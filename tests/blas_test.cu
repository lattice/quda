#include <stdio.h>
#include <stdlib.h>

#include <quda_internal.h>
#include <color_spinor_field.h>
#include <blas_quda.h>

#include <test_util.h>

#define Nkernels 23

QudaPrecision cuda_prec;
QudaPrecision other_prec; // Used for copy benchmark
cudaColorSpinorField *x, *y, *z, *w, *v, *p;

int nIters;

int Nthreads = 5;
int Ngrids = 11;
int blockSizes[] = {64, 128, 256, 512, 1024};
int gridSizes[] = {64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536};

int prec;

int LX = 24;
int LY = 24;
int LZ = 24;
int LT = 32;
int niter = 100 * 331776 / (LX * LY * LZ * LT); // 100 iterations on V=24^4

void init()
{
  ColorSpinorParam param;
  param.fieldLocation = QUDA_CUDA_FIELD_LOCATION;
  param.nColor = 3;
  param.nSpin = 4; // =1 for staggered, =2 for coarse Dslash, =4 for 4d spinor
  param.nDim = 4; // number of spacetime dimensions
  param.x[0] = LX;
  param.x[1] = LY;
  param.x[2] = LZ;
  param.x[3] = LT;
  param.pad = 0;
  param.siteSubset = QUDA_PARITY_SITE_SUBSET;
  param.siteOrder = QUDA_EVEN_ODD_SITE_ORDER;
  
  param.gammaBasis = QUDA_UKQCD_GAMMA_BASIS;
  param.create = QUDA_NULL_FIELD_CREATE;

  switch(prec) {
  case 0:
    param.precision = QUDA_HALF_PRECISION;
    other_prec = QUDA_SINGLE_PRECISION;
    param.fieldOrder = QUDA_FLOAT4_FIELD_ORDER;
    break;
  case 1:
    param.precision = QUDA_SINGLE_PRECISION;
    other_prec = QUDA_HALF_PRECISION;
    param.fieldOrder = QUDA_FLOAT4_FIELD_ORDER;
    break;
  case 2:
    param.precision = QUDA_DOUBLE_PRECISION;
    other_prec = QUDA_HALF_PRECISION;
    param.fieldOrder = QUDA_FLOAT2_FIELD_ORDER;
    break;
  }

  v = new cudaColorSpinorField(param);
  checkCudaError();

  w = new cudaColorSpinorField(param);
  x = new cudaColorSpinorField(param);
  y = new cudaColorSpinorField(param);
  z = new cudaColorSpinorField(param);

  param.precision = QUDA_SINGLE_PRECISION;
  param.fieldOrder = QUDA_FLOAT4_FIELD_ORDER; // always true since this is never double
  p = new cudaColorSpinorField(param);

  // check for successful allocation
  checkCudaError();

  // turn off error checking in blas kernels
  setBlasTuning(1);

}


void end()
{
  // release memory
  delete p;
  delete v;
  delete w;
  delete x;
  delete y;
  delete z;
}


double benchmark(int kernel) {

  double a, b, c;
  double2 a2, b2;

  cudaEvent_t start, end;
  cudaEventCreate(&start);
  cudaEventRecord(start, 0);
  cudaEventSynchronize(start);

  for (int i=0; i < nIters; ++i) {
    switch (kernel) {

    case 0:
      copyCuda(*y, *p);
      break;

    case 1:
      axpbyCuda(a, *x, b, *y);
      break;

    case 2:
      xpyCuda(*x, *y);
      break;

    case 3:
      axpyCuda(a, *x, *y);
      break;

    case 4:
      xpayCuda(*x, a, *y);
      break;

    case 5:
      mxpyCuda(*x, *y);
      break;

    case 6:
      axCuda(a, *x);
      break;

    case 7:
      caxpyCuda(a2, *x, *y);
      break;

    case 8:
      caxpbyCuda(a2, *x, b2, *y);
      break;

    case 9:
      cxpaypbzCuda(*x, a2, *y, b2, *z);
      break;

    case 10:
      axpyBzpcxCuda(a, *x, *y, b, *z, c);
      break;

    case 11:
      axpyZpbxCuda(a, *x, *y, *z, b);
      break;

    case 12:
      caxpbypzYmbwCuda(a2, *x, b2, *y, *z, *w);
      break;
      
      // double
    case 13:
      sumCuda(*x);
      break;

    case 14:
      normCuda(*x);
      break;

    case 15:
      reDotProductCuda(*x, *y);
      break;

    case 16:
      axpyNormCuda(a, *x, *y);
      break;

    case 17:
      xmyNormCuda(*x, *y);
      break;
      
      // double2
    case 18:
      cDotProductCuda(*x, *y);
      break;

    case 19:
      xpaycDotzyCuda(*x, a, *y, *z);
      break;
      
      // double3
    case 20:
      cDotProductNormACuda(*x, *y);
      break;

    case 21:
      cDotProductNormBCuda(*x, *y);
      break;

    case 22:
      caxpbypzYmbwcDotProductWYNormYCuda(a2, *x, b2, *y, *z, *w, *v);
      break;
      
    default:
      printf("Undefined blas kernel %d\n", kernel);
      exit(1);
    }
  }
  
  cudaEventCreate(&end);
  cudaEventRecord(end, 0);
  cudaEventSynchronize(end);
  float runTime;
  cudaEventElapsedTime(&runTime, start, end);
  cudaEventDestroy(start);
  cudaEventDestroy(end);

  double secs = runTime / 1000;
  return secs;
}


void write(char *names[], int threads[][3], int blocks[][3])
{
  printf("\nWriting optimal parameters to blas_param.h\n");

  FILE *fp = fopen("blas_param.h", "w");
  fprintf(fp, "//\n// Auto-tuned blas CUDA parameters, generated by blas_test\n//\n\n");

  fprintf(fp, "static int blas_threads[%d][3] = {\n", Nkernels);

  for (int i=0; i<Nkernels; i++) {
    fprintf(fp, "  {%4d, %4d, %4d}%c  // Kernel %2d: %s\n", threads[i][0], threads[i][1], threads[i][2],
	    ((i == Nkernels-1) ? ' ' : ','), i, names[i]);
  }
  fprintf(fp, "};\n\n");

  fprintf(fp, "static int blas_blocks[%d][3] = {\n", Nkernels);

  for (int i=0; i<Nkernels; i++) {
    fprintf(fp, "  {%5d, %5d, %5d}%c  // Kernel %2d: %s\n", blocks[i][0], blocks[i][1], blocks[i][2],
	    ((i == Nkernels-1) ? ' ' : ','), i, names[i]);
  }
  fprintf(fp, "};\n");

  fclose(fp);
}


int main(int argc, char** argv)
{
  int dev = 0;
  if (argc == 2) dev = atoi(argv[1]);
  initQuda(dev);

  int threads[Nkernels][3];
  int blocks[Nkernels][3];

  int kernels[] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22};
  char *names[] = {
    "copyCuda",
    "axpbyCuda",
    "xpyCuda",
    "axpyCuda",
    "xpayCuda",
    "mxpyCuda",
    "axCuda",
    "caxpyCuda",
    "caxpbyCuda",
    "cxpaypbzCuda",
    "axpyBzpcxCudax",
    "axpyZpbxCuda",
    "caxpbypzYmbwCuda",
    "sumCuda",
    "normCuda",
    "reDotProductCuda",
    "axpyNormCuda",
    "xmyNormCuda",
    "cDotProductCuda",
    "xpaycDotzyCuda",
    "cDotProductNormACuda",
    "cDotProductNormBCuda",
    "caxpbypzYmbwcDotProductWYNormYQuda"
  };
  
  // Only benchmark double precision if supported
#if (__CUDA_ARCH__ >= 130)
  int Nprec = 3;
#else
  int Nprec = 2;
#endif

  for (prec = 0; prec < Nprec; prec++) {

    init();

    printf("\nBenchmarking %d bit precision\n", (int)(pow(2.0,prec)*16));

    for (int i = 0; i < Nkernels; i++) {
 
      double gflops_max = 0.0;
      double gbytes_max = 0.0;
      int threads_max = 0; 
      int blocks_max = 0;

      cudaError_t error;

      for (int thread = 0; thread < Nthreads; thread++) {
	for (int grid = 0; grid < Ngrids; grid++) {
	  setBlasParam(i, prec, blockSizes[thread], gridSizes[grid]);

	  // first do warmup run
	  nIters = 1;
	  benchmark(kernels[i]);
	  
	  nIters = niter;
	  blas_quda_flops = 0;
	  blas_quda_bytes = 0;

	  double secs = benchmark(kernels[i]);
	  error = cudaGetLastError();
	  double flops = blas_quda_flops;
	  double bytes = blas_quda_bytes;
	  
	  double gflops = (flops*1e-9)/(secs);
	  double gbytes = bytes/(secs*(1<<30));

	  if (gbytes > gbytes_max && error == cudaSuccess) { // prevents selection of failed parameters
	    gflops_max = gflops;
	    gbytes_max = gbytes;
	    threads_max = blockSizes[thread];
	    blocks_max = gridSizes[grid];
	  }
	  
	  //printf("%d %d %-36s %f s, flops = %e, Gflops/s = %f, GiB/s = %f\n\n", 
	  //    blockSizes[thread], gridSizes[grid], names[i], secs, flops, gflops, gbytes);
	}
      }

      if (threads_max == 0) {
	errorQuda("Autotuning failed for %s kernel: %s", names[i], cudaGetErrorString(error));
      } else {
	
      }

      printf("%-32s: %d threads per block, %d blocks per grid, Gflops/s = %f, GiB/s = %f\n", 
	     names[i], threads_max, blocks_max, gflops_max, gbytes_max);

      threads[i][prec] = threads_max;
      blocks[i][prec] = blocks_max;
    }
    end();
  }
  write(names, threads, blocks);
  endQuda();
}


