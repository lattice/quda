
#include <stdlib.h>
#include <stdio.h>

#include <quda_internal.h>
#include <blas_quda.h>
#include <color_spinor_field.h>

#define REDUCE_MAX_BLOCKS 4096

#define REDUCE_DOUBLE 64
#define REDUCE_KAHAN 32

#if (__CUDA_ARCH__ == 130)
#define REDUCE_TYPE REDUCE_DOUBLE
#define QudaSumFloat double
#define QudaSumComplex cuDoubleComplex
#define QudaSumFloat3 double3
#else
#define REDUCE_TYPE REDUCE_KAHAN
#define QudaSumFloat float
#define QudaSumComplex cuComplex
#define QudaSumFloat3 float3
#endif

// Required for the reduction kernels
#ifdef __DEVICE_EMULATION__
#define EMUSYNC __syncthreads()
#else
#define EMUSYNC
#endif

// These are used for reduction kernels
static QudaSumFloat *d_reduceFloat=0;
static QudaSumComplex *d_reduceComplex=0;
static QudaSumFloat3 *d_reduceFloat3=0;

static QudaSumFloat *h_reduceFloat=0;
static QudaSumComplex *h_reduceComplex=0;
static QudaSumFloat3 *h_reduceFloat3=0;

unsigned long long blas_quda_flops;
unsigned long long blas_quda_bytes;

static dim3 blasBlock;
static dim3 blasGrid;

// generated by blas_test
#include <blas_param.h>

void zeroCuda(cudaColorSpinorField &a) { a.zero(); }

void initBlas(void)
{  
  if (!d_reduceFloat) {
    if (cudaMalloc((void**) &d_reduceFloat, REDUCE_MAX_BLOCKS*sizeof(QudaSumFloat)) == cudaErrorMemoryAllocation) {
      errorQuda("Error allocating device reduction array");
    }
  }

  if (!d_reduceComplex) {
    if (cudaMalloc((void**) &d_reduceComplex, REDUCE_MAX_BLOCKS*sizeof(QudaSumComplex)) == cudaErrorMemoryAllocation) {
      errorQuda("Error allocating device reduction array");
    }
  }
  
  if (!d_reduceFloat3) {
    if (cudaMalloc((void**) &d_reduceFloat3, REDUCE_MAX_BLOCKS*sizeof(QudaSumFloat3)) == cudaErrorMemoryAllocation) {
      errorQuda("Error allocating device reduction array");
    }
  }

  if (!h_reduceFloat) {
    if (cudaMallocHost((void**) &h_reduceFloat, REDUCE_MAX_BLOCKS*sizeof(QudaSumFloat)) == cudaErrorMemoryAllocation) {
      errorQuda("Error allocating host reduction array");
    }
  }

  if (!h_reduceComplex) {
    if (cudaMallocHost((void**) &h_reduceComplex, REDUCE_MAX_BLOCKS*sizeof(QudaSumComplex)) == cudaErrorMemoryAllocation) {
      errorQuda("Error allocating host reduction array");
    }
  }
  
  if (!h_reduceFloat3) {
    if (cudaMallocHost((void**) &h_reduceFloat3, REDUCE_MAX_BLOCKS*sizeof(QudaSumFloat3)) == cudaErrorMemoryAllocation) {
      errorQuda("Error allocating host reduction array");
    }
  }
}

void endBlas(void)
{
  if (d_reduceFloat) cudaFree(d_reduceFloat);
  if (d_reduceComplex) cudaFree(d_reduceComplex);
  if (d_reduceFloat3) cudaFree(d_reduceFloat3);
  if (h_reduceFloat) cudaFreeHost(h_reduceFloat);
  if (h_reduceComplex) cudaFreeHost(h_reduceComplex);
  if (h_reduceFloat3) cudaFreeHost(h_reduceFloat3);
}

// blasTuning = 1 turns off error checking
static int blasTuning = 0;

void setBlasTuning(int tuning)
{
  blasTuning = tuning;
}

void setBlasParam(int kernel, int prec, int threads, int blocks)
{
  blas_threads[kernel][prec] = threads;
  blas_blocks[kernel][prec] = blocks;
}

void setBlock(int kernel, int length, QudaPrecision precision)
{
  int prec;
  switch(precision) {
  case QUDA_HALF_PRECISION:
    prec = 0;
    break;
  case QUDA_SINGLE_PRECISION:
    prec = 1;
    break;
  case QUDA_DOUBLE_PRECISION:
    prec = 2;
    break;
  }

  int blocks = min(blas_blocks[kernel][prec], max(length/blas_threads[kernel][prec], 1));
  blasBlock.x = blas_threads[kernel][prec];
  blasBlock.y = 1;
  blasBlock.z = 1;

  blasGrid.x = blocks;
  blasGrid.y = 1;
  blasGrid.z = 1;
}

#if (__CUDA_ARCH__ == 130)
static __inline__ __device__ double2 fetch_double2(texture<int4, 1> t, int i)
{
  int4 v = tex1Dfetch(t,i);
  return make_double2(__hiloint2double(v.y, v.x), __hiloint2double(v.w, v.z));
}
#else
static __inline__ __device__ double2 fetch_double2(texture<int4, 1> t, int i)
{
  // do nothing
  return make_double2(0.0, 0.0);
}
#endif

float2 __device__ read_Float2(float2 *x, int i) {
  return make_float2(x[i].x, x[i].y);
}

double2 __device__ read_Float2(double2 *x, int i) {
  return make_double2(x[i].x, x[i].y);
}

#define READ_DOUBLE2_TEXTURE(x, i) \
  fetch_double2(x##TexDouble2, i)

#define READ_FLOAT2_TEXTURE(x, i) \
  tex1Dfetch(x##TexSingle2, i)

float2 __device__ make_Float2(float2 x) {
  return make_float2(x.x, x.y);
}

double2 __device__ make_Float2(double2 x) {
  return make_double2(x.x, x.y);
}

#define RECONSTRUCT_HALF_SPINOR(a, texHalf, texNorm, length)		\
  float4 a##0 = tex1Dfetch(texHalf, i + 0*length);			\
  float4 a##1 = tex1Dfetch(texHalf, i + 1*length);			\
  float4 a##2 = tex1Dfetch(texHalf, i + 2*length);			\
  float4 a##3 = tex1Dfetch(texHalf, i + 3*length);			\
  float4 a##4 = tex1Dfetch(texHalf, i + 4*length);			\
  float4 a##5 = tex1Dfetch(texHalf, i + 5*length);			\
  {float b = tex1Dfetch(texNorm, i);					\
  (a##0).x *= b; (a##0).y *= b; (a##0).z *= b; (a##0).w *= b;		\
  (a##1).x *= b; (a##1).y *= b; (a##1).z *= b; (a##1).w *= b;		\
  (a##2).x *= b; (a##2).y *= b; (a##2).z *= b; (a##2).w *= b;		\
  (a##3).x *= b; (a##3).y *= b; (a##3).z *= b; (a##3).w *= b;		\
  (a##4).x *= b; (a##4).y *= b; (a##4).z *= b; (a##4).w *= b;		\
  (a##5).x *= b; (a##5).y *= b; (a##5).z *= b; (a##5).w *= b;}

#define READ_HALF_SPINOR(a, tex, length)				\
  float4 a##0 = tex1Dfetch(tex, i + 0*length);				\
  float4 a##1 = tex1Dfetch(tex, i + 1*length);				\
  float4 a##2 = tex1Dfetch(tex, i + 2*length);				\
  float4 a##3 = tex1Dfetch(tex, i + 3*length);				\
  float4 a##4 = tex1Dfetch(tex, i + 4*length);				\
  float4 a##5 = tex1Dfetch(tex, i + 5*length);				\
  float a##c = a[i];

#define SHORT_LENGTH 65536
#define SCALE_FLOAT ((SHORT_LENGTH-1) * 0.5)
#define SHIFT_FLOAT (-1.f / (SHORT_LENGTH-1))

__device__ short float2short(float c, float a) {
  //return (short)(a*MAX_SHORT);
  short rtn = (short)((a+SHIFT_FLOAT)*SCALE_FLOAT*c);
  return rtn;
}

__device__ float short2float(short a) {
  return (float)a/SCALE_FLOAT - SHIFT_FLOAT;
}

__device__ short4 float42short4(float c, float4 a) {
  return make_short4(float2short(c, a.x), float2short(c, a.y), float2short(c, a.z), float2short(c, a.w));
}

#define CONSTRUCT_HALF_SPINOR_FROM_SINGLE(h, n, a, length)		\
  {float c0 = fmaxf(fabsf((a##0).x), fabsf((a##0).y));			\
  float c1 = fmaxf(fabsf((a##0).z), fabsf((a##0).w));			\
  float c2 = fmaxf(fabsf((a##1).x), fabsf((a##1).y));			\
  float c3 = fmaxf(fabsf((a##1).z), fabsf((a##1).w));			\
  float c4 = fmaxf(fabsf((a##2).x), fabsf((a##2).y));			\
  float c5 = fmaxf(fabsf((a##2).z), fabsf((a##2).w));			\
  float c6 = fmaxf(fabsf((a##3).x), fabsf((a##3).y));			\
  float c7 = fmaxf(fabsf((a##3).z), fabsf((a##3).w));			\
  float c8 = fmaxf(fabsf((a##4).x), fabsf((a##4).y));			\
  float c9 = fmaxf(fabsf((a##4).z), fabsf((a##4).w));			\
  float c10 = fmaxf(fabsf((a##5).x), fabsf((a##5).y));			\
  float c11 = fmaxf(fabsf((a##5).z), fabsf((a##5).w));			\
  c0 = fmaxf(c0, c1); c1 = fmaxf(c2, c3); c2 = fmaxf(c4, c5);		\
  c3 = fmaxf(c6, c7); c4 = fmaxf(c8, c9); c5 = fmaxf(c10, c11);		\
  c0 = fmaxf(c0, c1); c1 = fmaxf(c2, c3); c2 = fmaxf(c4, c5);		\
  c0 = fmaxf(c0, c1); c0 = fmaxf(c0, c2);				\
  n[i] = c0;								\
  float C = __fdividef(MAX_SHORT, c0);					\
  h[i+0*length] = make_short4((short)(C*(float)(a##0).x), (short)(C*(float)(a##0).y), \
			      (short)(C*(float)(a##0).z), (short)(C*(float)(a##0).w)); \
  h[i+1*length] = make_short4((short)(C*(float)(a##1).x), (short)(C*(float)(a##1).y), \
			      (short)(C*(float)(a##1).z), (short)(C*(float)(a##1).w)); \
  h[i+2*length] = make_short4((short)(C*(float)(a##2).x), (short)(C*(float)(a##2).y), \
			      (short)(C*(float)(a##2).z), (short)(C*(float)(a##2).w)); \
  h[i+3*length] = make_short4((short)(C*(float)(a##3).x), (short)(C*(float)(a##3).y), \
			      (short)(C*(float)(a##3).z), (short)(C*(float)(a##3).w)); \
  h[i+4*length] = make_short4((short)(C*(float)(a##4).x), (short)(C*(float)(a##4).y), \
			      (short)(C*(float)(a##4).z), (short)(C*(float)(a##4).w)); \
  h[i+5*length] = make_short4((short)(C*(float)(a##5).x), (short)(C*(float)(a##5).y),	\
  (short)(C*(float)(a##5).z), (short)(C*(float)(a##5).w));}

  /*
  float C = 1.0f / c0;							\
  h[i+0*length] = float42short4(C, a##0);				\
  h[i+1*length] = float42short4(C, a##1);				\
  h[i+2*length] = float42short4(C, a##2);				\
  h[i+3*length] = float42short4(C, a##3);				\
  h[i+4*length] = float42short4(C, a##4);				\
  h[i+5*length] = float42short4(C, a##5);}
*/

#define CONSTRUCT_HALF_SPINOR_FROM_DOUBLE(h, n, a, length)		\
  {float c0 = fmaxf(fabsf((a##0).x), fabsf((a##0).y));			\
  float c1 = fmaxf(fabsf((a##1).x), fabsf((a##1).y));		     	\
  float c2 = fmaxf(fabsf((a##2).x), fabsf((a##2).y));			\
  float c3 = fmaxf(fabsf((a##3).x), fabsf((a##3).y));			\
  float c4 = fmaxf(fabsf((a##4).x), fabsf((a##4).y));			\
  float c5 = fmaxf(fabsf((a##5).x), fabsf((a##5).y));			\
  float c6 = fmaxf(fabsf((a##6).x), fabsf((a##6).y));			\
  float c7 = fmaxf(fabsf((a##7).x), fabsf((a##7).y));			\
  float c8 = fmaxf(fabsf((a##8).x), fabsf((a##8).y));			\
  float c9 = fmaxf(fabsf((a##9).x), fabsf((a##9).y));			\
  float c10 = fmaxf(fabsf((a##10).x), fabsf((a##10).y));		\
  float c11 = fmaxf(fabsf((a##11).x), fabsf((a##11).y));		\
  c0 = fmaxf(c0, c1); c1 = fmaxf(c2, c3);  c2 = fmaxf(c4, c5); c3 = fmaxf(c6, c7); \
  c4 = fmaxf(c8, c9); c5 = fmaxf(c10, c11); c0 = fmaxf(c0, c1); c1 = fmaxf(c2, c3); \
  c2 = fmaxf(c4, c5); c0 = fmaxf(c0, c1); c0 = fmaxf(c0, c2);		\
  n[i] = c0;								\
  float C = __fdividef(MAX_SHORT, c0);					\
  h[i+0*length] = make_short4((short)(C*(float)(a##0).x), (short)(C*(float)(a##0).y), \
			      (short)(C*(float)(a##1).x), (short)(C*(float)(a##1).y)); \
  h[i+1*length] = make_short4((short)(C*(float)(a##2).x), (short)(C*(float)(a##2).y), \
			      (short)(C*(float)(a##3).x), (short)(C*(float)(a##3).y)); \
  h[i+2*length] = make_short4((short)(C*(float)(a##4).x), (short)(C*(float)(a##4).y), \
			      (short)(C*(float)(a##5).x), (short)(C*(float)(a##5).y)); \
  h[i+3*length] = make_short4((short)(C*(float)(a##6).x), (short)(C*(float)(a##6).y), \
			      (short)(C*(float)(a##7).x), (short)(C*(float)(a##7).y)); \
  h[i+4*length] = make_short4((short)(C*(float)(a##8).x), (short)(C*(float)(a##8).y), \
			      (short)(C*(float)(a##9).x), (short)(C*(float)(a##9).y)); \
  h[i+5*length] = make_short4((short)(C*(float)(a##10).x), (short)(C*(float)(a##10).y),	\
			      (short)(C*(float)(a##11).x), (short)(C*(float)(a##11).y));}

#define SUM_FLOAT4(sum, a)			\
  float sum = a.x + a.y + a.z + a.w;

#define REAL_DOT_FLOAT4(dot, a, b) \
  float dot = a.x*b.x + a.y*b.y + a.z*b.z + a.w*b.w

#define IMAG_DOT_FLOAT4(dot, a, b) \
  float dot = a.x*b.y - a.y*b.x + a.z*b.w - a.w*b.z

#define AX_FLOAT4(a, X)				\
  X.x *= a; X.y *= a; X.z *= a; X.w *= a;

#define XPY_FLOAT4(X, Y)		     \
  Y.x += X.x; Y.y += X.y; Y.z += X.z; Y.w += X.w;

#define XMY_FLOAT4(X, Y)		     \
  Y.x = X.x - Y.x; Y.y = X.y - X.y; Y.z = X.z - Y.z; Y.w = X.w - Y.w;

#define MXPY_FLOAT4(X, Y)		     \
  Y.x -= X.x; Y.y -= X.y; Y.z -= X.z; Y.w -= X.w;

#define AXPY_FLOAT4(a, X, Y)		     \
  Y.x += a*X.x;	Y.y += a*X.y;		     \
  Y.z += a*X.z;	Y.w += a*X.w;

#define AXPBY_FLOAT4(a, X, b, Y)		\
  Y.x = a*X.x + b*Y.x; Y.y = a*X.y + b*Y.y;	\
  Y.z = a*X.z + b*Y.z; Y.w = a*X.w + b*Y.w;

#define XPAY_FLOAT4(X, a, Y)			     \
  Y.x = X.x + a*Y.x; Y.y = X.y + a*Y.y;		     \
  Y.z = X.z + a*Y.z; Y.w = X.w + a*Y.w;

#define CAXPY_FLOAT4(a, X, Y) \
  Y.x += a.x*X.x - a.y*X.y;   \
  Y.y += a.y*X.x + a.x*X.y;   \
  Y.z += a.x*X.z - a.y*X.w;   \
  Y.w += a.y*X.z + a.x*X.w;

#define CMAXPY_FLOAT4(a, X, Y)			\
  Y.x -= (a.x*X.x - a.y*X.y);			\
  Y.y -= (a.y*X.x + a.x*X.y);			\
  Y.z -= (a.x*X.z - a.y*X.w);			\
  Y.w -= (a.y*X.z + a.x*X.w);

#define CAXPBY_FLOAT4(a, X, b, Y)		 \
  Y.x = a.x*X.x - a.y*X.y + b.x*Y.x - b.y*Y.y;   \
  Y.y = a.y*X.x + a.x*X.y + b.y*Y.x + b.x*Y.y;   \
  Y.z = a.x*X.z - a.y*X.w + b.x*Y.z - b.y*Y.w;   \
  Y.w = a.y*X.z + a.x*X.w + b.y*Y.z + b.x*Y.w;

#define CXPAYPBZ_FLOAT4(X, a, Y, b, Z)		\
  {float2 z;					       \
  z.x = X.x + a.x*Y.x - a.y*Y.y + b.x*Z.x - b.y*Z.y;   \
  z.y = X.y + a.y*Y.x + a.x*Y.y + b.y*Z.x + b.x*Z.y;   \
  Z.x = z.x; Z.y = z.y;				       \
  z.x = X.z + a.x*Y.z - a.y*Y.w + b.x*Z.z - b.y*Z.w;   \
  z.y = X.w + a.y*Y.z + a.x*Y.w + b.y*Z.z + b.x*Z.w;   \
  Z.z = z.x; Z.w = z.y;}

#define CAXPBYPZ_FLOAT4(a, X, b, Y, Z)		  \
  Z.x += a.x*X.x - a.y*X.y + b.x*Y.x - b.y*Y.y;   \
  Z.y += a.y*X.x + a.x*X.y + b.y*Y.x + b.x*Y.y;   \
  Z.z += a.x*X.z - a.y*X.w + b.x*Y.z - b.y*Y.w;   \
  Z.w += a.y*X.z + a.x*X.w + b.y*Y.z + b.x*Y.w;

// Double precision input spinor field
texture<int4, 1> xTexDouble2;
texture<int4, 1> yTexDouble2;
texture<int4, 1> zTexDouble2;
texture<int4, 1> wTexDouble2;
texture<int4, 1> uTexDouble2;

// Single precision input spinor field
texture<float2, 1> xTexSingle2;
texture<float2, 1> yTexSingle2;

texture<float4, 1> xTexSingle4;

// Half precision input spinor field
texture<short4, 1, cudaReadModeNormalizedFloat> texHalf1;
texture<float, 1, cudaReadModeElementType> texNorm1;

// Half precision input spinor field
texture<short4, 1, cudaReadModeNormalizedFloat> texHalf2;
texture<float, 1, cudaReadModeElementType> texNorm2;

// Half precision input spinor field
texture<short4, 1, cudaReadModeNormalizedFloat> texHalf3;
texture<float, 1, cudaReadModeElementType> texNorm3;

// Half precision input spinor field
texture<short4, 1, cudaReadModeNormalizedFloat> texHalf4;
texture<float, 1, cudaReadModeElementType> texNorm4;

// Half precision input spinor field
texture<short4, 1, cudaReadModeNormalizedFloat> texHalf5;
texture<float, 1, cudaReadModeElementType> texNorm5;

inline void checkSpinor(cudaColorSpinorField &a, cudaColorSpinorField &b) {
  if (a.Precision() != b.Precision()) {
    errorQuda("checkSpinor: precisions do not match: %d %d", a.Precision(), b.Precision());
  }

  if (a.Length() != b.Length()) {
    errorQuda("checkSpinor: lengths do not match: %d %d", a.Length(), b.Length());
  }

  if (a.Stride() != b.Stride()) {
    errorQuda("checkSpinor: strides do not match: %d %d", a.Stride(), b.Stride());
  }
}

// For kernels with precision conversion built in
inline void checkSpinorLength(cudaColorSpinorField &a, cudaColorSpinorField &b) {
  if (a.Length() != b.Length()) {
    errorQuda("checkSpinor: lengths do not match: %d %d", a.Length(), b.Length());
  }
}

__global__ void convertDSKernel(double2 *dst, float4 *src, int length) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < length) {
    for (int k=0; k<6; k++) {
      dst[2*k*length+i].x = src[k*length+i].x;
      dst[2*k*length+i].y = src[k*length+i].y;
      dst[(2*k+1)*length+i].x = src[k*length+i].z;
      dst[(2*k+1)*length+i].y = src[k*length+i].w;
    }
    i += gridSize;
  }   
}

__global__ void convertSDKernel(float4 *dst, double2 *src, int length) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < length) {
    for (int k=0; k<6; k++) {
      dst[k*length+i].x = src[2*k*length+i].x;
      dst[k*length+i].y = src[2*k*length+i].y;
      dst[k*length+i].z = src[(2*k+1)*length+i].x;
      dst[k*length+i].w = src[(2*k+1)*length+i].y;
    }
    i += gridSize;
  }   
}

__global__ void convertHSKernel(short4 *h, float *norm, int length, int real_length) {

  int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;

  while(i < real_length) {
    float4 F0 = tex1Dfetch(xTexSingle4, i + 0*length);
    float4 F1 = tex1Dfetch(xTexSingle4, i + 1*length);
    float4 F2 = tex1Dfetch(xTexSingle4, i + 2*length);
    float4 F3 = tex1Dfetch(xTexSingle4, i + 3*length);
    float4 F4 = tex1Dfetch(xTexSingle4, i + 4*length);
    float4 F5 = tex1Dfetch(xTexSingle4, i + 5*length);
    CONSTRUCT_HALF_SPINOR_FROM_SINGLE(h, norm, F, length);
    i += gridSize;
  }

}

__global__ void convertSHKernel(float4 *res, int length, int real_length) {

  int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;

  while (i<real_length) {
    RECONSTRUCT_HALF_SPINOR(I, texHalf1, texNorm1, length);
    res[0*length+i] = I0;
    res[1*length+i] = I1;
    res[2*length+i] = I2;
    res[3*length+i] = I3;
    res[4*length+i] = I4;
    res[5*length+i] = I5;
    i += gridSize;
  }
}

__global__ void convertHDKernel(short4 *h, float *norm, int length, int real_length) {

  int i = blockIdx.x*(blockDim.x) + threadIdx.x; 
  unsigned int gridSize = gridDim.x*blockDim.x;

  while(i < real_length) {
    double2 F0 = fetch_double2(xTexDouble2, i+0*length);
    double2 F1 = fetch_double2(xTexDouble2, i+1*length);
    double2 F2 = fetch_double2(xTexDouble2, i+2*length);
    double2 F3 = fetch_double2(xTexDouble2, i+3*length);
    double2 F4 = fetch_double2(xTexDouble2, i+4*length);
    double2 F5 = fetch_double2(xTexDouble2, i+5*length);
    double2 F6 = fetch_double2(xTexDouble2, i+6*length);
    double2 F7 = fetch_double2(xTexDouble2, i+7*length);
    double2 F8 = fetch_double2(xTexDouble2, i+8*length);
    double2 F9 = fetch_double2(xTexDouble2, i+9*length);
    double2 F10 = fetch_double2(xTexDouble2, i+10*length);
    double2 F11 = fetch_double2(xTexDouble2, i+11*length);
    CONSTRUCT_HALF_SPINOR_FROM_DOUBLE(h, norm, F, length);
    i += gridSize;
  }
}

__global__ void convertDHKernel(double2 *res, int length, int real_length) {

  int i = blockIdx.x*(blockDim.x) + threadIdx.x; 
  unsigned int gridSize = gridDim.x*blockDim.x;

  while(i < real_length) {
    RECONSTRUCT_HALF_SPINOR(I, texHalf1, texNorm1, length);
    res[0*length+i] = make_double2(I0.x, I0.y);
    res[1*length+i] = make_double2(I0.z, I0.w);
    res[2*length+i] = make_double2(I1.x, I1.y);
    res[3*length+i] = make_double2(I1.z, I1.w);
    res[4*length+i] = make_double2(I2.x, I2.y);
    res[5*length+i] = make_double2(I2.z, I2.w);
    res[6*length+i] = make_double2(I3.x, I3.y);
    res[7*length+i] = make_double2(I3.z, I3.w);
    res[8*length+i] = make_double2(I4.x, I4.y);
    res[9*length+i] = make_double2(I4.z, I4.w);
    res[10*length+i] = make_double2(I5.x, I5.y);
    res[11*length+i] = make_double2(I5.z, I5.w);
    i += gridSize;
  }

}

void copyCuda(cudaColorSpinorField &dst, const cudaColorSpinorField &src) {
  setBlock(0, dst.stride, dst.Precision());

  blas_quda_bytes += src.real_length*(src.precision + dst.precision);

  if (dst.precision == QUDA_DOUBLE_PRECISION && src.precision == QUDA_SINGLE_PRECISION) {
    convertDSKernel<<<blasGrid, blasBlock>>>((double2*)dst.v, (float4*)src.v, src.stride);
  } else if (dst.precision == QUDA_SINGLE_PRECISION && src.precision == QUDA_DOUBLE_PRECISION) {
    convertSDKernel<<<blasGrid, blasBlock>>>((float4*)dst.v, (double2*)src.v, src.stride);
  } else if (dst.precision == QUDA_SINGLE_PRECISION && src.precision == QUDA_HALF_PRECISION) {
    int spinor_bytes = dst.length*sizeof(short);
    cudaBindTexture(0, texHalf1, src.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, src.norm, spinor_bytes/12);
    convertSHKernel<<<blasGrid, blasBlock>>>((float4*)dst.v, src.stride, src.volume);
  } else if (dst.precision == QUDA_HALF_PRECISION && src.precision == QUDA_SINGLE_PRECISION) {
    int spinor_bytes = dst.length*sizeof(float);
    cudaBindTexture(0, xTexSingle4, src.v, spinor_bytes); 
    convertHSKernel<<<blasGrid, blasBlock>>>((short4*)dst.v, (float*)dst.norm, src.stride, src.volume);
  } else if (dst.precision == QUDA_DOUBLE_PRECISION && src.precision == QUDA_HALF_PRECISION) {
    int spinor_bytes = dst.length*sizeof(short);
    cudaBindTexture(0, texHalf1, src.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, src.norm, spinor_bytes/12);
    convertDHKernel<<<blasGrid, blasBlock>>>((double2*)dst.v, src.stride, src.volume);
  } else if (dst.precision == QUDA_HALF_PRECISION && src.precision == QUDA_DOUBLE_PRECISION) {
    int spinor_bytes = dst.length*sizeof(double);
    cudaBindTexture(0, xTexDouble2, src.v, spinor_bytes); 
    convertHDKernel<<<blasGrid, blasBlock>>>((short4*)dst.v, (float*)dst.norm, src.stride, src.volume);
  } else {
    cudaMemcpy(dst.v, src.v, dst.bytes, cudaMemcpyDeviceToDevice);
    if (dst.precision == QUDA_HALF_PRECISION)
      cudaMemcpy(dst.norm, src.norm, dst.bytes/(dst.nColor*dst.nSpin), cudaMemcpyDeviceToDevice);
  }
}


template <typename Float>
__global__ void axpbyKernel(Float a, Float *x, Float b, Float *y, int length) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < length) {
    y[i] = a*x[i] + b*y[i];
    i += gridSize;
  } 
}

__global__ void axpbyHKernel(float a, float b, short4 *yH, float *yN, int stride, int length) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < length) {
    RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);
    RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);
    AXPBY_FLOAT4(a, x0, b, y0);
    AXPBY_FLOAT4(a, x1, b, y1);
    AXPBY_FLOAT4(a, x2, b, y2);
    AXPBY_FLOAT4(a, x3, b, y3);
    AXPBY_FLOAT4(a, x4, b, y4);
    AXPBY_FLOAT4(a, x5, b, y5);
    CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);
    i += gridSize;
  } 
  
}

// performs the operation y[i] = a*x[i] + b*y[i]
void axpbyCuda(const double &a, cudaColorSpinorField &x, const double &b, cudaColorSpinorField &y) {
  setBlock(1, x.length, x.precision);
  checkSpinor(x, y);
  if (x.precision == QUDA_DOUBLE_PRECISION) {
    axpbyKernel<<<blasGrid, blasBlock>>>(a, (double*)x.v, b, (double*)y.v, x.length);
  } else if (x.precision == QUDA_SINGLE_PRECISION) {
    axpbyKernel<<<blasGrid, blasBlock>>>((float)a, (float*)x.v, (float)b, (float*)y.v, x.length);
  } else {
    int spinor_bytes = x.length*sizeof(short);
    cudaBindTexture(0, texHalf1, x.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, x.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf2, y.v, spinor_bytes); 
    cudaBindTexture(0, texNorm2, y.norm, spinor_bytes/12);    
    axpbyHKernel<<<blasGrid, blasBlock>>>((float)a, (float)b, (short4*)y.v, 
					(float*)y.norm, y.stride, y.volume);
    blas_quda_bytes += (3*x.real_length*x.precision) / (x.nColor * x.nSpin);
  }
  blas_quda_bytes += 3*x.real_length*x.precision;
  blas_quda_flops += 3*x.real_length;
}

template <typename Float>
__global__ void xpyKernel(Float *x, Float *y, int len) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < len) {
    y[i] += x[i];
    i += gridSize;
  } 
}

__global__ void xpyHKernel(short4 *yH, float *yN, int stride, int length) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < length) {
    RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);
    RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);
    XPY_FLOAT4(x0, y0);
    XPY_FLOAT4(x1, y1);
    XPY_FLOAT4(x2, y2);
    XPY_FLOAT4(x3, y3);
    XPY_FLOAT4(x4, y4);
    XPY_FLOAT4(x5, y5);
    CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);
    i += gridSize;
  } 
  
}

// performs the operation y[i] = x[i] + y[i]
void xpyCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  checkSpinor(x,y);
  setBlock(2, x.length, x.precision);
  if (x.precision == QUDA_DOUBLE_PRECISION) {
    xpyKernel<<<blasGrid, blasBlock>>>((double*)x.v, (double*)y.v, x.length);
  } else if (x.precision == QUDA_SINGLE_PRECISION) {
    xpyKernel<<<blasGrid, blasBlock>>>((float*)x.v, (float*)y.v, x.length);
  } else {
    int spinor_bytes = x.length*sizeof(short);
    cudaBindTexture(0, texHalf1, x.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, x.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf2, y.v, spinor_bytes); 
    cudaBindTexture(0, texNorm2, y.norm, spinor_bytes/12);    
    xpyHKernel<<<blasGrid, blasBlock>>>((short4*)y.v, (float*)y.norm, y.stride, y.volume);
    blas_quda_bytes += (3*x.real_length*x.precision) / (x.nColor * x.nSpin);
  }
  blas_quda_bytes += 3*x.real_length*x.precision;
  blas_quda_flops += x.real_length;
}

template <typename Float>
__global__ void axpyKernel(Float a, Float *x, Float *y, int len) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < len) {
    y[i] += a*x[i];
    i += gridSize;
  } 
}

__global__ void axpyHKernel(float a, short4 *yH, float *yN, int stride, int length) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < length) {
    RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);
    RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);
    AXPY_FLOAT4(a, x0, y0);
    AXPY_FLOAT4(a, x1, y1);
    AXPY_FLOAT4(a, x2, y2);
    AXPY_FLOAT4(a, x3, y3);
    AXPY_FLOAT4(a, x4, y4);
    AXPY_FLOAT4(a, x5, y5);
    CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);
    i += gridSize;
  } 
  
}

// performs the operation y[i] = a*x[i] + y[i]
void axpyCuda(const double &a, cudaColorSpinorField &x, cudaColorSpinorField &y) {
  checkSpinor(x,y);
  setBlock(3, x.length, x.precision);
  if (x.precision == QUDA_DOUBLE_PRECISION) {
    axpyKernel<<<blasGrid, blasBlock>>>(a, (double*)x.v, (double*)y.v, x.length);
  } else if (x.precision == QUDA_SINGLE_PRECISION) {
    axpyKernel<<<blasGrid, blasBlock>>>((float)a, (float*)x.v, (float*)y.v, x.length);
  } else {
    int spinor_bytes = x.length*sizeof(short);
    cudaBindTexture(0, texHalf1, x.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, x.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf2, y.v, spinor_bytes); 
    cudaBindTexture(0, texNorm2, y.norm, spinor_bytes/12);    
    axpyHKernel<<<blasGrid, blasBlock>>>((float)a, (short4*)y.v, (float*)y.norm, y.stride, y.volume);
    blas_quda_bytes += (3*x.real_length*x.precision) / (x.nColor * x.nSpin);
  }
  blas_quda_bytes += 3*x.real_length*x.precision;
  blas_quda_flops += 2*x.real_length;
}

template <typename Float>
__global__ void xpayKernel(Float *x, Float a, Float *y, int len) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < len) {
    y[i] = x[i] + a*y[i];
    i += gridSize;
  } 
}

__global__ void xpayHKernel(float a, short4 *yH, float *yN, int stride, int length) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < length) {
    RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);
    RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);
    XPAY_FLOAT4(x0, a, y0);
    XPAY_FLOAT4(x1, a, y1);
    XPAY_FLOAT4(x2, a, y2);
    XPAY_FLOAT4(x3, a, y3);
    XPAY_FLOAT4(x4, a, y4);
    XPAY_FLOAT4(x5, a, y5);
    CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);
    i += gridSize;
  } 
  
}

// performs the operation y[i] = x[i] + a*y[i]
void xpayCuda(cudaColorSpinorField &x, const double &a, cudaColorSpinorField &y) {
  checkSpinor(x,y);
  setBlock(4, x.length, x.precision);
  if (x.precision == QUDA_DOUBLE_PRECISION) {
    xpayKernel<<<blasGrid, blasBlock>>>((double*)x.v, a, (double*)y.v, x.length);
  } else if (x.precision == QUDA_SINGLE_PRECISION) {
    xpayKernel<<<blasGrid, blasBlock>>>((float*)x.v, (float)a, (float*)y.v, x.length);
  } else {
    int spinor_bytes = x.length*sizeof(short);
    cudaBindTexture(0, texHalf1, x.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, x.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf2, y.v, spinor_bytes); 
    cudaBindTexture(0, texNorm2, y.norm, spinor_bytes/12);    
    xpayHKernel<<<blasGrid, blasBlock>>>((float)a, (short4*)y.v, (float*)y.norm, y.stride, y.volume);
    blas_quda_bytes += (3*x.real_length*x.precision) / (x.nColor * x.nSpin);
  }
  blas_quda_bytes += 3*x.real_length*x.precision;
  blas_quda_flops += 2*x.real_length;
}

template <typename Float>
__global__ void mxpyKernel(Float *x, Float *y, int len) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < len) {
    y[i] -= x[i];
    i += gridSize;
  } 
}

__global__ void mxpyHKernel(short4 *yH, float *yN, int stride, int length) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < length) {
    RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);
    RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);
    MXPY_FLOAT4(x0, y0);
    MXPY_FLOAT4(x1, y1);
    MXPY_FLOAT4(x2, y2);
    MXPY_FLOAT4(x3, y3);
    MXPY_FLOAT4(x4, y4);
    MXPY_FLOAT4(x5, y5);
    CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);
    i += gridSize;
  } 
  
}


// performs the operation y[i] -= x[i] (minus x plus y)
void mxpyCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  checkSpinor(x,y);
  setBlock(5, x.length, x.precision);
  if (x.precision == QUDA_DOUBLE_PRECISION) {
    mxpyKernel<<<blasGrid, blasBlock>>>((double*)x.v, (double*)y.v, x.length);
  } else if (x.precision == QUDA_SINGLE_PRECISION) {
    mxpyKernel<<<blasGrid, blasBlock>>>((float*)x.v, (float*)y.v, x.length);
  } else {
    int spinor_bytes = x.length*sizeof(short);
    cudaBindTexture(0, texHalf1, x.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, x.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf2, y.v, spinor_bytes); 
    cudaBindTexture(0, texNorm2, y.norm, spinor_bytes/12);    
    mxpyHKernel<<<blasGrid, blasBlock>>>((short4*)y.v, (float*)y.norm, y.stride, y.volume);
    blas_quda_bytes += (3*x.real_length*x.precision) / (x.nColor * x.nSpin);
  }
  blas_quda_bytes += 3*x.real_length*x.precision;
  blas_quda_flops += x.real_length;
}

template <typename Float>
__global__ void axKernel(Float a, Float *x, int len) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < len) {
    x[i] *= a;
    i += gridSize;
  } 
}

template <typename Float, typename Float2>
__global__ void ax2Kernel(Float a, Float2 *x, int len) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < len) {
    x[i].x *= a;
    x[i].y *= a;
    i += gridSize;
  } 
}

__global__ void axHKernel(float a, short4 *xH, float *xN, int stride, int length) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < length) {
    RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);
    AX_FLOAT4(a, x0); AX_FLOAT4(a, x1); AX_FLOAT4(a, x2);
    AX_FLOAT4(a, x3); AX_FLOAT4(a, x4); AX_FLOAT4(a, x5);
    CONSTRUCT_HALF_SPINOR_FROM_SINGLE(xH, xN, x, stride);
    i += gridSize;
  } 
  
}

// performs the operation x[i] = a*x[i]
void axCuda(const double &a, cudaColorSpinorField &x) {
  setBlock(6, x.length, x.precision);
  if (x.precision == QUDA_DOUBLE_PRECISION) {
    axKernel<<<blasGrid, blasBlock>>>(a, (double*)x.v, x.length);
  } else if (x.precision == QUDA_SINGLE_PRECISION) {
    ax2Kernel<<<blasGrid, blasBlock>>>((float)a, (float2*)x.v, x.length/2);
  } else {
    int spinor_bytes = x.length*sizeof(short);
    cudaBindTexture(0, texHalf1, x.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, x.norm, spinor_bytes/12);    
    axHKernel<<<blasGrid, blasBlock>>>((float)a, (short4*)x.v, (float*)x.norm, x.stride, x.volume);
    blas_quda_bytes += (2*x.real_length*x.precision) / (x.nColor * x.nSpin);
  }
  blas_quda_bytes += 2*x.real_length*x.precision;
  blas_quda_flops += x.real_length;
}

template <typename Float2>
__global__ void caxpyDKernel(Float2 a, Float2 *x, Float2 *y, int len) {
  
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < len) {
    Float2 Z = READ_DOUBLE2_TEXTURE(x, i);
    y[i].x += a.x*Z.x - a.y*Z.y;
    y[i].y += a.y*Z.x + a.x*Z.y;
    i += gridSize;
  } 
  
}

template <typename Float2>
__global__ void caxpySKernel(Float2 a, Float2 *x, Float2 *y, int len) {
  
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < len) {
    Float2 Z = read_Float2(x, i);
    y[i].x += a.x*Z.x - a.y*Z.y;
    y[i].y += a.y*Z.x + a.x*Z.y;
    i += gridSize;
  } 
  
}

__global__ void caxpyHKernel(float2 a, short4 *yH, float *yN, int stride, int length) {
  
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < length) {
    RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);
    RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);
    CAXPY_FLOAT4(a, x0, y0);
    CAXPY_FLOAT4(a, x1, y1);
    CAXPY_FLOAT4(a, x2, y2);
    CAXPY_FLOAT4(a, x3, y3);
    CAXPY_FLOAT4(a, x4, y4);
    CAXPY_FLOAT4(a, x5, y5);
    CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);
    i += gridSize;
  } 
  
}

// performs the operation y[i] += a*x[i]
void caxpyCuda(const double2 &a, cudaColorSpinorField &x, cudaColorSpinorField &y) {
  checkSpinor(x,y);
  int length = x.length/2;
  setBlock(7, length, x.precision);
  blas_quda_bytes += 3*x.real_length*x.precision;
  blas_quda_flops += 4*x.real_length;
  if (x.precision == QUDA_DOUBLE_PRECISION) {
    int spinor_bytes = x.length*sizeof(double);
    cudaBindTexture(0, xTexDouble2, x.v, spinor_bytes); 
    cudaBindTexture(0, yTexDouble2, y.v, spinor_bytes); 
    caxpyDKernel<<<blasGrid, blasBlock>>>(a, (double2*)x.v, (double2*)y.v, length);
  } else if (x.precision == QUDA_SINGLE_PRECISION) {
    float2 af2 = make_float2((float)a.x, (float)a.y);
    caxpySKernel<<<blasGrid, blasBlock>>>(af2, (float2*)x.v, (float2*)y.v, length);
  } else {
    int spinor_bytes = x.length*sizeof(short);
    cudaBindTexture(0, texHalf1, x.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, x.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf2, y.v, spinor_bytes); 
    cudaBindTexture(0, texNorm2, y.norm, spinor_bytes/12);    
    float2 af2 = make_float2((float)a.x, (float)a.y);
    caxpyHKernel<<<blasGrid, blasBlock>>>(af2, (short4*)y.v, (float*)y.norm, y.stride, y.volume);
    blas_quda_bytes += (3*x.real_length*x.precision) / (x.nColor * x.nSpin);
  }
}

template <typename Float2>
__global__ void caxpbyDKernel(Float2 a, Float2 *x, Float2 b, Float2 *y, int len) {
  
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < len) {
    Float2 Z1 = READ_DOUBLE2_TEXTURE(x, i);
    Float2 Z2 = READ_DOUBLE2_TEXTURE(y, i);
    y[i].x = a.x*Z1.x + b.x*Z2.x - a.y*Z1.y - b.y*Z2.y;
    y[i].y = a.y*Z1.x + b.y*Z2.x + a.x*Z1.y + b.x*Z2.y;
    i += gridSize;
  } 
}

template <typename Float2>
__global__ void caxpbySKernel(Float2 a, Float2 *x, Float2 b, Float2 *y, int len) {
  
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < len) {
    Float2 Z1 = read_Float2(x, i);
    Float2 Z2 = read_Float2(y, i);
    y[i].x = a.x*Z1.x + b.x*Z2.x - a.y*Z1.y - b.y*Z2.y;
    y[i].y = a.y*Z1.x + b.y*Z2.x + a.x*Z1.y + b.x*Z2.y;
    i += gridSize;
  } 
}

__global__ void caxpbyHKernel(float2 a, float2 b, short4 *yH, float *yN, int stride, int length) {
  
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < length) {
    RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);
    RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);
    CAXPBY_FLOAT4(a, x0, b, y0);
    CAXPBY_FLOAT4(a, x1, b, y1);
    CAXPBY_FLOAT4(a, x2, b, y2);
    CAXPBY_FLOAT4(a, x3, b, y3);
    CAXPBY_FLOAT4(a, x4, b, y4);
    CAXPBY_FLOAT4(a, x5, b, y5);
    CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);
    i += gridSize;
  }   
}


// performs the operation y[i] = c*x[i] + b*y[i]
void caxpbyCuda(const double2 &a, cudaColorSpinorField &x, const double2 &b, cudaColorSpinorField &y) {
  checkSpinor(x,y);
  int length = x.length/2;
  setBlock(8, length, x.precision);
  blas_quda_bytes += 3*x.real_length*x.precision;
  blas_quda_flops += 7*x.real_length;
  if (x.precision == QUDA_DOUBLE_PRECISION) {
    int spinor_bytes = x.length*sizeof(double);
    cudaBindTexture(0, xTexDouble2, x.v, spinor_bytes); 
    cudaBindTexture(0, yTexDouble2, y.v, spinor_bytes); 
    caxpbyDKernel<<<blasGrid, blasBlock>>>(a, (double2*)x.v, b, (double2*)y.v, length);
  } else if (x.precision == QUDA_SINGLE_PRECISION) {
    float2 af2 = make_float2((float)a.x, (float)a.y);
    float2 bf2 = make_float2((float)b.x, (float)b.y);
    caxpbySKernel<<<blasGrid, blasBlock>>>(af2, (float2*)x.v, bf2, (float2*)y.v, length);
  } else {
    int spinor_bytes = x.length*sizeof(short);
    cudaBindTexture(0, texHalf1, x.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, x.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf2, y.v, spinor_bytes); 
    cudaBindTexture(0, texNorm2, y.norm, spinor_bytes/12);    
    float2 af2 = make_float2((float)a.x, (float)a.y);
    float2 bf2 = make_float2((float)b.x, (float)b.y);
    caxpbyHKernel<<<blasGrid, blasBlock>>>(af2, bf2, (short4*)y.v, (float*)y.norm, y.stride, y.volume);
    blas_quda_bytes += (3*x.real_length*x.precision) / (x.nColor * x.nSpin);
  }
}

template <typename Float2>
__global__ void cxpaypbzDKernel(Float2 *x, Float2 a, Float2 *y, Float2 b, Float2 *z, int len) {
  
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < len) {
    Float2 T1 = READ_DOUBLE2_TEXTURE(x, i);
    Float2 T2 = READ_DOUBLE2_TEXTURE(y, i);
    Float2 T3 = read_Float2(z, i);

    T1.x += a.x*T2.x - a.y*T2.y;
    T1.y += a.y*T2.x + a.x*T2.y;
    T1.x += b.x*T3.x - b.y*T3.y;
    T1.y += b.y*T3.x + b.x*T3.y;
    
    z[i] = make_Float2(T1);
    i += gridSize;
  } 
  
}

template <typename Float2>
__global__ void cxpaypbzSKernel(Float2 *x, Float2 a, Float2 *y, Float2 b, Float2 *z, int len) {
  
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < len) {
    Float2 T1 = read_Float2(x, i);
    Float2 T2 = read_Float2(y, i);
    Float2 T3 = read_Float2(z, i);

    T1.x += a.x*T2.x - a.y*T2.y;
    T1.y += a.y*T2.x + a.x*T2.y;
    T1.x += b.x*T3.x - b.y*T3.y;
    T1.y += b.y*T3.x + b.x*T3.y;
    
    z[i] = make_Float2(T1);
    i += gridSize;
  } 
  
}

__global__ void cxpaypbzHKernel(float2 a, float2 b, short4 *zH, float *zN, int stride, int length) {
  
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < length) {
    RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);
    RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);
    RECONSTRUCT_HALF_SPINOR(z, texHalf3, texNorm3, stride);
    CXPAYPBZ_FLOAT4(x0, a, y0, b, z0);
    CXPAYPBZ_FLOAT4(x1, a, y1, b, z1);
    CXPAYPBZ_FLOAT4(x2, a, y2, b, z2);
    CXPAYPBZ_FLOAT4(x3, a, y3, b, z3);
    CXPAYPBZ_FLOAT4(x4, a, y4, b, z4);
    CXPAYPBZ_FLOAT4(x5, a, y5, b, z5);
    CONSTRUCT_HALF_SPINOR_FROM_SINGLE(zH, zN, z, stride);
    i += gridSize;
  }   
}


// performs the operation z[i] = x[i] + a*y[i] + b*z[i]
void cxpaypbzCuda(cudaColorSpinorField &x, const double2 &a, cudaColorSpinorField &y, const double2 &b, cudaColorSpinorField &z) {
  checkSpinor(x,y);
  checkSpinor(x,z);
  int length = x.length/2;
  setBlock(9, length, x.precision);
  blas_quda_bytes += 4*x.real_length*x.precision;
  blas_quda_flops += 8*x.real_length;
  if (x.precision == QUDA_DOUBLE_PRECISION) {
    int spinor_bytes = x.length*sizeof(double);
    cudaBindTexture(0, xTexDouble2, x.v, spinor_bytes); 
    cudaBindTexture(0, yTexDouble2, y.v, spinor_bytes); 
    cxpaypbzDKernel<<<blasGrid, blasBlock>>>((double2*)x.v, a, (double2*)y.v, b, (double2*)z.v, length);
  } else if (x.precision == QUDA_SINGLE_PRECISION) {
    float2 af2 = make_float2((float)a.x, (float)a.y);
    float2 bf2 = make_float2((float)b.x, (float)b.y);
    cxpaypbzSKernel<<<blasGrid, blasBlock>>>((float2*)x.v, af2, (float2*)y.v, bf2, (float2*)z.v, length);
  } else {
    int spinor_bytes = x.length*sizeof(short);
    cudaBindTexture(0, texHalf1, x.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, x.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf2, y.v, spinor_bytes); 
    cudaBindTexture(0, texNorm2, y.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf3, z.v, spinor_bytes); 
    cudaBindTexture(0, texNorm3, z.norm, spinor_bytes/12);    
    float2 af2 = make_float2((float)a.x, (float)a.y);
    float2 bf2 = make_float2((float)b.x, (float)b.y);
    cxpaypbzHKernel<<<blasGrid, blasBlock>>>(af2, bf2, (short4*)z.v, (float*)z.norm, z.stride, z.volume);
    blas_quda_bytes += (4*x.real_length*x.precision) / (x.nColor * x.nSpin);
  }
}

template <typename Float, typename Float2>
__global__ void axpyZpbxDKernel(Float a, Float2 *x, Float2 *y, Float2 *z, Float b, int len) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < len) {
    Float2 x_i = READ_DOUBLE2_TEXTURE(x, i);
    Float2 z_i = READ_DOUBLE2_TEXTURE(z, i);
    y[i].x += a*x_i.x;
    y[i].y += a*x_i.y;
    x[i].x = z_i.x + b*x_i.x;
    x[i].y = z_i.y + b*x_i.y;
    i += gridSize;
  }
}

template <typename Float, typename Float2>
__global__ void axpyZpbxSKernel(Float a, Float2 *x, Float2 *y, Float2 *z, Float b, int len) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < len) {
    Float2 x_i = read_Float2(x, i);
    y[i].x += a*x_i.x;
    y[i].y += a*x_i.y;
    x[i].x = z[i].x + b*x_i.x;
    x[i].y = z[i].y + b*x_i.y;
    i += gridSize;
  }
}

__global__ void axpyZpbxHKernel(float a, float b, short4 *xH, float *xN, short4 *yH, float *yN, int stride, int length) {
  
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < length) {
    RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);
    RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);
    RECONSTRUCT_HALF_SPINOR(z, texHalf3, texNorm3, stride);
    AXPY_FLOAT4(a, x0, y0);
    XPAY_FLOAT4(z0, b, x0);
    AXPY_FLOAT4(a, x1, y1);
    XPAY_FLOAT4(z1, b, x1);
    AXPY_FLOAT4(a, x2, y2);
    XPAY_FLOAT4(z2, b, x2);
    AXPY_FLOAT4(a, x3, y3);
    XPAY_FLOAT4(z3, b, x3);
    AXPY_FLOAT4(a, x4, y4);
    XPAY_FLOAT4(z4, b, x4);
    AXPY_FLOAT4(a, x5, y5);
    XPAY_FLOAT4(z5, b, x5);
    CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);
    CONSTRUCT_HALF_SPINOR_FROM_SINGLE(xH, xN, x, stride);
    i += gridSize;
  }   
}


// performs the operations: {y[i] = a*x[i] + y[i]; x[i] = z[i] + b*x[i]}
void axpyZpbxCuda(const double &a, cudaColorSpinorField &x, cudaColorSpinorField &y, cudaColorSpinorField &z, const double &b) {
  checkSpinor(x,y);
  checkSpinor(x,z);
  setBlock(10, x.length, x.precision);
  if (x.precision == QUDA_DOUBLE_PRECISION) {
    int spinor_bytes = x.length*sizeof(double);
    cudaBindTexture(0, xTexDouble2, x.v, spinor_bytes); 
    cudaBindTexture(0, zTexDouble2, z.v, spinor_bytes); 
    axpyZpbxDKernel<<<blasGrid, blasBlock>>>(a, (double2*)x.v, (double2*)y.v, (double2*)z.v, b, x.length/2);
  } else if (x.precision == QUDA_SINGLE_PRECISION) {
    axpyZpbxSKernel<<<blasGrid, blasBlock>>>((float)a, (float2*)x.v, (float2*)y.v, (float2*)z.v, (float)b, x.length/2);
  } else {
    int spinor_bytes = x.length*sizeof(short);
    cudaBindTexture(0, texHalf1, x.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, x.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf2, y.v, spinor_bytes); 
    cudaBindTexture(0, texNorm2, y.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf3, z.v, spinor_bytes); 
    cudaBindTexture(0, texNorm3, z.norm, spinor_bytes/12);    
    axpyZpbxHKernel<<<blasGrid, blasBlock>>>((float)a, (float)b, (short4*)x.v, (float*)x.norm,
					   (short4*)y.v, (float*)y.norm, z.stride, z.volume);
    blas_quda_bytes += (5*x.real_length*x.precision) / (x.nColor * x.nSpin);
  }
  blas_quda_bytes += 5*x.real_length*x.precision;
  blas_quda_flops += 8*x.real_length;
}

template <typename Float2>
__global__ void caxpbypzYmbwDKernel(Float2 a, Float2 *x, Float2 b, Float2 *y, Float2 *z, Float2 *w, int len) {

  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < len) {
    Float2 X = READ_DOUBLE2_TEXTURE(x, i);
    Float2 Z = read_Float2(z, i);

    Z.x += a.x*X.x - a.y*X.y;
    Z.y += a.y*X.x + a.x*X.y;

    Float2 Y = READ_DOUBLE2_TEXTURE(y, i);
    Z.x += b.x*Y.x - b.y*Y.y;
    Z.y += b.y*Y.x + b.x*Y.y;
    z[i] = make_Float2(Z);

    Float2 W = read_Float2(w, i);

    Y.x -= b.x*W.x - b.y*W.y;
    Y.y -= b.y*W.x + b.x*W.y;	
    
    y[i] = make_Float2(Y);
    i += gridSize;
  } 
}

template <typename Float2>
__global__ void caxpbypzYmbwSKernel(Float2 a, Float2 *x, Float2 b, Float2 *y, Float2 *z, Float2 *w, int len) {

  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < len) {
    Float2 X = read_Float2(x, i);
    Float2 Z = read_Float2(z, i);

    Z.x += a.x*X.x - a.y*X.y;
    Z.y += a.y*X.x + a.x*X.y;

    Float2 Y = read_Float2(y, i);
    Z.x += b.x*Y.x - b.y*Y.y;
    Z.y += b.y*Y.x + b.x*Y.y;
    z[i] = make_Float2(Z);

    Float2 W = read_Float2(w, i);

    Y.x -= b.x*W.x - b.y*W.y;
    Y.y -= b.y*W.x + b.x*W.y;	
    
    y[i] = make_Float2(Y);
    i += gridSize;
  } 
}

__global__ void caxpbypzYmbwHKernel(float2 a, float2 b, short4 *yH, float *yN, short4 *zH, float *zN, int stride, int length) {
  
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < length) {
    RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);
    RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);
    RECONSTRUCT_HALF_SPINOR(z, texHalf3, texNorm3, stride);
    RECONSTRUCT_HALF_SPINOR(w, texHalf4, texNorm4, stride);
    CAXPBYPZ_FLOAT4(a, x0, b, y0, z0);
    CAXPBYPZ_FLOAT4(a, x1, b, y1, z1);
    CAXPBYPZ_FLOAT4(a, x2, b, y2, z2);
    CAXPBYPZ_FLOAT4(a, x3, b, y3, z3);
    CAXPBYPZ_FLOAT4(a, x4, b, y4, z4);
    CAXPBYPZ_FLOAT4(a, x5, b, y5, z5);
    CMAXPY_FLOAT4(b, w0, y0);
    CMAXPY_FLOAT4(b, w1, y1);
    CMAXPY_FLOAT4(b, w2, y2);
    CMAXPY_FLOAT4(b, w3, y3);
    CMAXPY_FLOAT4(b, w4, y4);
    CMAXPY_FLOAT4(b, w5, y5);
    CONSTRUCT_HALF_SPINOR_FROM_SINGLE(zH, zN, z, stride);
    CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);
    i += gridSize;
  }   
}

// performs the operation z[i] = a*x[i] + b*y[i] + z[i] and y[i] -= b*w[i]
void caxpbypzYmbwCuda(const double2 &a, cudaColorSpinorField &x, const double2 &b, cudaColorSpinorField &y,
                      cudaColorSpinorField &z, cudaColorSpinorField &w) {
  checkSpinor(x,y);
  checkSpinor(x,z);
  checkSpinor(x,w);
  int length = x.length/2;
  setBlock(11, length, x.precision);
  if (x.precision == QUDA_DOUBLE_PRECISION) {
    int spinor_bytes = x.length*sizeof(double);
    cudaBindTexture(0, xTexDouble2, x.v, spinor_bytes); 
    cudaBindTexture(0, yTexDouble2, y.v, spinor_bytes); 
    cudaBindTexture(0, zTexDouble2, z.v, spinor_bytes); 
    caxpbypzYmbwDKernel<<<blasGrid, blasBlock>>>(a, (double2*)x.v, b, (double2*)y.v, 
					  (double2*)z.v, (double2*)w.v, length); 
  } else if (x.precision == QUDA_SINGLE_PRECISION) {
    float2 af2 = make_float2((float)a.x, (float)a.y);
    float2 bf2 = make_float2((float)b.x, (float)b.y);
    caxpbypzYmbwSKernel<<<blasGrid, blasBlock>>>(af2, (float2*)x.v, bf2, (float2*)y.v, 
					  (float2*)z.v, (float2*)w.v, length); 
  } else {
    int spinor_bytes = x.length*sizeof(short);
    cudaBindTexture(0, texHalf1, x.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, x.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf2, y.v, spinor_bytes); 
    cudaBindTexture(0, texNorm2, y.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf3, z.v, spinor_bytes); 
    cudaBindTexture(0, texNorm3, z.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf4, w.v, spinor_bytes); 
    cudaBindTexture(0, texNorm4, w.norm, spinor_bytes/12); 
    float2 af2 = make_float2((float)a.x, (float)a.y);
    float2 bf2 = make_float2((float)b.x, (float)b.y);
    caxpbypzYmbwHKernel<<<blasGrid, blasBlock>>>(af2, bf2, (short4*)y.v, (float*)y.norm,
					       (short4*)z.v, (float*)z.norm, z.stride, z.volume);
    blas_quda_bytes += (6*x.real_length*x.precision) / (x.nColor * x.nSpin);
  }
  blas_quda_bytes += 6*x.real_length*x.precision;
  blas_quda_flops += 12*x.real_length;
}


// Computes c = a + b in "double single" precision.
__device__ void dsadd(QudaSumFloat &c0, QudaSumFloat &c1, const QudaSumFloat a0, 
		      const QudaSumFloat a1, const float b0, const float b1) {
  // Compute dsa + dsb using Knuth's trick.
  QudaSumFloat t1 = a0 + b0;
  QudaSumFloat e = t1 - a0;
  QudaSumFloat t2 = ((b0 - e) + (a0 - (t1 - e))) + a1 + b1;
  // The result is t1 + t2, after normalization.
  c0 = e = t1 + t2;
  c1 = t2 - (e - t1);
}

// Computes c = a + b in "double single" precision (complex version)
__device__ void zcadd(QudaSumComplex &c0, QudaSumComplex &c1, const QudaSumComplex a0, 
		      const QudaSumComplex a1, const QudaSumComplex b0, const QudaSumComplex b1) {
  // Compute dsa + dsb using Knuth's trick.
  QudaSumFloat t1 = a0.x + b0.x;
  QudaSumFloat e = t1 - a0.x;
  QudaSumFloat t2 = ((b0.x - e) + (a0.x - (t1 - e))) + a1.x + b1.x;
  // The result is t1 + t2, after normalization.
  c0.x = e = t1 + t2;
  c1.x = t2 - (e - t1);
  
  // Compute dsa + dsb using Knuth's trick.
  t1 = a0.y + b0.y;
  e = t1 - a0.y;
  t2 = ((b0.y - e) + (a0.y - (t1 - e))) + a1.y + b1.y;
  // The result is t1 + t2, after normalization.
  c0.y = e = t1 + t2;
  c1.y = t2 - (e - t1);
}

// Computes c = a + b in "double single" precision (float3 version)
__device__ void dsadd3(QudaSumFloat3 &c0, QudaSumFloat3 &c1, const QudaSumFloat3 a0, 
		       const QudaSumFloat3 a1, const QudaSumFloat3 b0, const QudaSumFloat3 b1) {
  // Compute dsa + dsb using Knuth's trick.
  QudaSumFloat t1 = a0.x + b0.x;
  QudaSumFloat e = t1 - a0.x;
  QudaSumFloat t2 = ((b0.x - e) + (a0.x - (t1 - e))) + a1.x + b1.x;
  // The result is t1 + t2, after normalization.
  c0.x = e = t1 + t2;
  c1.x = t2 - (e - t1);
  
  // Compute dsa + dsb using Knuth's trick.
  t1 = a0.y + b0.y;
  e = t1 - a0.y;
  t2 = ((b0.y - e) + (a0.y - (t1 - e))) + a1.y + b1.y;
  // The result is t1 + t2, after normalization.
  c0.y = e = t1 + t2;
  c1.y = t2 - (e - t1);
  
  // Compute dsa + dsb using Knuth's trick.
  t1 = a0.z + b0.z;
  e = t1 - a0.z;
  t2 = ((b0.z - e) + (a0.z - (t1 - e))) + a1.z + b1.z;
  // The result is t1 + t2, after normalization.
  c0.z = e = t1 + t2;
  c1.z = t2 - (e - t1);
}

//
// double sumCuda(float *a, int n) {}
//
template <int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) sumD##suffix
#define REDUCE_TYPES Float *a
#define REDUCE_PARAMS a
#define REDUCE_AUXILIARY(i)
#define REDUCE_OPERATION(i) a[i]
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

template <int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) sumS##suffix
#define REDUCE_TYPES Float *a
#define REDUCE_PARAMS a
#define REDUCE_AUXILIARY(i)
#define REDUCE_OPERATION(i) a[i].x + a[i].y
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

template <int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) sumH##suffix
#define REDUCE_TYPES Float *a, int stride
#define REDUCE_PARAMS a, stride
#define REDUCE_AUXILIARY(i)						\
  READ_HALF_SPINOR(a, texHalf1, stride);				\
  SUM_FLOAT4(s0, a0);							\
  SUM_FLOAT4(s1, a1);							\
  SUM_FLOAT4(s2, a2);							\
  SUM_FLOAT4(s3, a3);							\
  SUM_FLOAT4(s4, a4);							\
  SUM_FLOAT4(s5, a5);							\
  s0 += s1; s2 += s3; s4 += s5; s0 += s2; s0 += s4;
#define REDUCE_OPERATION(i) (ac*s0)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

double sumCuda(cudaColorSpinorField &a) {
  blas_quda_flops += a.real_length;
  blas_quda_bytes += a.real_length*a.precision;
  if (a.precision == QUDA_DOUBLE_PRECISION) {
    return sumDCuda((double*)a.v, a.length, 12, a.precision);
  } else if (a.precision == QUDA_SINGLE_PRECISION) {
    return sumSCuda((float2*)a.v, a.length/2, 12, a.precision);
  } else {
    int spinor_bytes = a.length*sizeof(short);
    cudaBindTexture(0, texHalf1, a.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, a.norm, spinor_bytes/12);    
    blas_quda_bytes += (a.real_length*a.precision) / (a.nColor * a.nSpin);
    return sumHCuda((float*)a.norm, a.stride, a.volume, 12, a.precision);
  }
}

//
// double normCuda(float *a, int n) {}
//
template <int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) normD##suffix
#define REDUCE_TYPES Float *a
#define REDUCE_PARAMS a
#define REDUCE_AUXILIARY(i)
#define REDUCE_OPERATION(i) (a[i]*a[i])
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

template <int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) normS##suffix
#define REDUCE_TYPES Float *a
#define REDUCE_PARAMS a
#define REDUCE_AUXILIARY(i)
#define REDUCE_OPERATION(i) (a[i].x*a[i].x + a[i].y*a[i].y)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

//
// double normHCuda(char *, int n) {}
//
template <int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) normH##suffix
#define REDUCE_TYPES Float *a, int stride // dummy type
#define REDUCE_PARAMS a, stride
#define REDUCE_AUXILIARY(i)						\
  READ_HALF_SPINOR(a, texHalf1, stride);				\
  REAL_DOT_FLOAT4(norm0, a0, a0);					\
  REAL_DOT_FLOAT4(norm1, a1, a1);					\
  REAL_DOT_FLOAT4(norm2, a2, a2);					\
  REAL_DOT_FLOAT4(norm3, a3, a3);					\
  REAL_DOT_FLOAT4(norm4, a4, a4);					\
  REAL_DOT_FLOAT4(norm5, a5, a5);					\
  norm0 += norm1; norm2 += norm3; norm4 += norm5; norm0 += norm2, norm0 += norm4;
#define REDUCE_OPERATION(i) (ac*ac*norm0)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

double normCuda(const cudaColorSpinorField &a) {
  blas_quda_flops += 2*a.real_length;
  blas_quda_bytes += a.real_length*a.precision;
  if (a.precision == QUDA_DOUBLE_PRECISION) {
    return normDCuda((double*)a.v, a.length, 13, a.precision);
  } else if (a.precision == QUDA_SINGLE_PRECISION) {
    return normSCuda((float2*)a.v, a.length/2, 13, a.precision);
  } else {
    int spinor_bytes = a.length*sizeof(short);
    cudaBindTexture(0, texHalf1, a.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, a.norm, spinor_bytes/12);    
    blas_quda_bytes += (2*a.real_length*a.precision) / (a.nColor * a.nSpin);
    return normHCuda((float*)a.norm, a.stride, a.volume, 13, a.precision);
  }
}



double normEven(const cudaColorSpinorField &a) {
  blas_quda_flops += 2*a.real_length;
  blas_quda_bytes += a.real_length*a.precision;
  if (a.precision == QUDA_DOUBLE_PRECISION) {
    return normDCuda((double*)a.v, a.length/2, 13, a.precision);
  } else if (a.precision == QUDA_SINGLE_PRECISION) {
    return normSCuda((float2*)a.v, a.length/4, 13, a.precision);
  } else {
    int spinor_bytes = a.length*sizeof(short);
    cudaBindTexture(0, texHalf1, a.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, a.norm, spinor_bytes/12);    
    blas_quda_bytes += (2*a.real_length*a.precision) / (a.nColor * a.nSpin);
    return normHCuda((float*)a.norm, a.stride/2, a.volume/2, 13, a.precision);
  }
}



//
// double reDotProductFCuda(float *a, float *b, int n) {}
//
template <int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) reDotProductD##suffix
#define REDUCE_TYPES Float *a, Float *b
#define REDUCE_PARAMS a, b
#define REDUCE_AUXILIARY(i)
#define REDUCE_OPERATION(i) (a[i]*b[i])
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

template <int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) reDotProductS##suffix
#define REDUCE_TYPES Float *a, Float *b
#define REDUCE_PARAMS a, b
#define REDUCE_AUXILIARY(i)
#define REDUCE_OPERATION(i) (a[i].x*b[i].x + a[i].y*b[i].y)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

//
// double reDotProductHCuda(float *a, float *b, int n) {}
//
template <int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) reDotProductH##suffix
#define REDUCE_TYPES Float *a, Float *b, int stride
#define REDUCE_PARAMS a, b, stride
#define REDUCE_AUXILIARY(i)						\
  READ_HALF_SPINOR(a, texHalf1, stride);				\
  READ_HALF_SPINOR(b, texHalf2, stride);				\
  REAL_DOT_FLOAT4(rdot0, a0, b0);					\
  REAL_DOT_FLOAT4(rdot1, a1, b1);					\
  REAL_DOT_FLOAT4(rdot2, a2, b2);					\
  REAL_DOT_FLOAT4(rdot3, a3, b3);					\
  REAL_DOT_FLOAT4(rdot4, a4, b4);					\
  REAL_DOT_FLOAT4(rdot5, a5, b5);					\
  rdot0 += rdot1; rdot2 += rdot3; rdot4 += rdot5; rdot0 += rdot2; rdot0 += rdot4;
#define REDUCE_OPERATION(i) (ac*bc*rdot0)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

double reDotProductCuda(cudaColorSpinorField &a, cudaColorSpinorField &b) {
  blas_quda_flops += 2*a.real_length;
  checkSpinor(a, b);
  blas_quda_bytes += 2*a.real_length*a.precision;
  if (a.precision == QUDA_DOUBLE_PRECISION) {
    return reDotProductDCuda((double*)a.v, (double*)b.v, a.length, 14, a.precision);
  } else if (a.precision == QUDA_SINGLE_PRECISION) {
    return reDotProductSCuda((float2*)a.v, (float2*)b.v, a.length/2, 14, a.precision);
  } else {
    int spinor_bytes = a.length*sizeof(short);
    cudaBindTexture(0, texHalf1, a.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, a.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf2, b.v, spinor_bytes); 
    cudaBindTexture(0, texNorm2, b.norm, spinor_bytes/12);    
    blas_quda_bytes += (2*a.real_length*a.precision) / (a.nColor * a.nSpin);
    return reDotProductHCuda((float*)a.norm, (float*)b.norm, a.stride, a.volume, 14, a.precision);
  }
}

//
// double axpyNormCuda(float a, float *x, float *y, n){}
//
// First performs the operation y[i] = a*x[i] + y[i]
// Second returns the norm of y
//

template <int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) axpyNormF##suffix
#define REDUCE_TYPES Float a, Float *x, Float *y
#define REDUCE_PARAMS a, x, y
#define REDUCE_AUXILIARY(i) y[i] = a*x[i] + y[i]
#define REDUCE_OPERATION(i) (y[i]*y[i])
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

template <int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) axpyNormH##suffix
#define REDUCE_TYPES Float a, short4 *yH, float *yN, int stride
#define REDUCE_PARAMS a, yH, yN, stride
#define REDUCE_AUXILIARY(i)						\
  RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);		\
  AXPY_FLOAT4(a, x0, y0);						\
  REAL_DOT_FLOAT4(norm0, y0, y0);					\
  AXPY_FLOAT4(a, x1, y1);						\
  REAL_DOT_FLOAT4(norm1, y1, y1);					\
  AXPY_FLOAT4(a, x2, y2);						\
  REAL_DOT_FLOAT4(norm2, y2, y2);					\
  AXPY_FLOAT4(a, x3, y3);						\
  REAL_DOT_FLOAT4(norm3, y3, y3);					\
  AXPY_FLOAT4(a, x4, y4);						\
  REAL_DOT_FLOAT4(norm4, y4, y4);					\
  AXPY_FLOAT4(a, x5, y5);						\
  REAL_DOT_FLOAT4(norm5, y5, y5);					\
  norm0 += norm1; norm2 += norm3; norm4 += norm5; norm0 += norm2; norm0 += norm4; \
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);
#define REDUCE_OPERATION(i) (norm0)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

double axpyNormCuda(const double &a, cudaColorSpinorField &x, cudaColorSpinorField &y) {
  blas_quda_flops += 4*x.real_length;
  checkSpinor(x,y);
  blas_quda_bytes += 3*x.real_length*x.precision;
  if (x.precision == QUDA_DOUBLE_PRECISION) {
    return axpyNormFCuda(a, (double*)x.v, (double*)y.v, x.length, 15, x.precision);
  } else if (x.precision == QUDA_SINGLE_PRECISION) {
    return axpyNormFCuda((float)a, (float*)x.v, (float*)y.v, x.length, 15, x.precision);
  } else {
    cudaBindTexture(0, texHalf1, x.v, x.bytes); 
    cudaBindTexture(0, texNorm1, x.norm, x.bytes/(x.nColor*x.nSpin));    
    cudaBindTexture(0, texHalf2, y.v, x.bytes); 
    cudaBindTexture(0, texNorm2, y.norm, x.bytes/(x.nColor*x.nSpin));    
    blas_quda_bytes += (3*x.real_length*x.precision) / (x.nColor * x.nSpin);
    return axpyNormHCuda((float)a, (short4*)y.v, (float*)y.norm, x.stride, x.volume, 15, x.precision);
  }
}

//
// double xmyNormCuda(float a, float *x, float *y, n){}
//
// First performs the operation y[i] = x[i] - y[i]
// Second returns the norm of y
//

template <int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) xmyNormF##suffix
#define REDUCE_TYPES Float *x, Float *y
#define REDUCE_PARAMS x, y
#define REDUCE_AUXILIARY(i) y[i] = x[i] - y[i]
#define REDUCE_OPERATION(i) (y[i]*y[i])
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

template <int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) xmyNormH##suffix
#define REDUCE_TYPES Float *d1, Float *d2, short4 *yH, float *yN, int stride
#define REDUCE_PARAMS d1, d2, yH, yN, stride
#define REDUCE_AUXILIARY(i)						\
  RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);		\
  XMY_FLOAT4(x0, y0);							\
  REAL_DOT_FLOAT4(norm0, y0, y0);					\
  XMY_FLOAT4(x1, y1);							\
  REAL_DOT_FLOAT4(norm1, y1, y1);					\
  XMY_FLOAT4(x2, y2);							\
  REAL_DOT_FLOAT4(norm2, y2, y2);					\
  XMY_FLOAT4(x3, y3);							\
  REAL_DOT_FLOAT4(norm3, y3, y3);					\
  XMY_FLOAT4(x4, y4);							\
  REAL_DOT_FLOAT4(norm4, y4, y4);					\
  XMY_FLOAT4(x5, y5);							\
  REAL_DOT_FLOAT4(norm5, y5, y5);					\
  norm0 += norm1; norm2 += norm3; norm4 += norm5; norm0 += norm2; norm0 += norm4; \
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);
#define REDUCE_OPERATION(i) (norm0)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

double xmyNormCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  blas_quda_flops += 3*x.real_length;
  checkSpinor(x,y);
  blas_quda_bytes += 3*x.real_length*x.precision;
  if (x.precision == QUDA_DOUBLE_PRECISION) {
    return xmyNormFCuda((double*)x.v, (double*)y.v, x.length, 16, x.precision);
  } else if (x.precision == QUDA_SINGLE_PRECISION) {
    return xmyNormFCuda((float*)x.v, (float*)y.v, x.length, 16, x.precision);
  } else { 
    cudaBindTexture(0, texHalf1, x.v, x.bytes); 
    cudaBindTexture(0, texNorm1, x.norm, x.bytes/(x.nColor*x.nSpin));    
    cudaBindTexture(0, texHalf2, y.v, x.bytes); 
    cudaBindTexture(0, texNorm2, y.norm, x.bytes/(x.nColor*x.nSpin));    
    blas_quda_bytes += (3*x.real_length*x.precision) / (x.nColor * x.nSpin);
    return xmyNormHCuda((char*)0, (char*)0, (short4*)y.v, (float*)y.norm, y.stride, y.volume, 16, x.precision);
  }
}


//
// double2 cDotProductCuda(float2 *x, float2 *y, int n) {}
//
template <int reduce_threads, typename Float, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductD##suffix
#define REDUCE_TYPES Float2 *x, Float2 *y, Float c
#define REDUCE_PARAMS x, y, c
#define REDUCE_REAL_AUXILIARY(i) Float2 a = READ_DOUBLE2_TEXTURE(x, i);
#define REDUCE_IMAG_AUXILIARY(i) Float2 b = READ_DOUBLE2_TEXTURE(y, i);
#define REDUCE_REAL_OPERATION(i) (a.x*b.x + a.y*b.y)
#define REDUCE_IMAG_OPERATION(i) (a.x*b.y - a.y*b.x)
#include "reduce_complex_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_REAL_AUXILIARY
#undef REDUCE_IMAG_AUXILIARY
#undef REDUCE_REAL_OPERATION
#undef REDUCE_IMAG_OPERATION

template <int reduce_threads, typename Float, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductS##suffix
#define REDUCE_TYPES Float2 *x, Float2 *y, Float c
#define REDUCE_PARAMS x, y, c
#define REDUCE_REAL_AUXILIARY(i) Float2 a = read_Float2(x, i);
#define REDUCE_IMAG_AUXILIARY(i) Float2 b = read_Float2(y, i);
#define REDUCE_REAL_OPERATION(i) (a.x*b.x + a.y*b.y)
#define REDUCE_IMAG_OPERATION(i) (a.x*b.y - a.y*b.x)
#include "reduce_complex_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_REAL_AUXILIARY
#undef REDUCE_IMAG_AUXILIARY
#undef REDUCE_REAL_OPERATION
#undef REDUCE_IMAG_OPERATION

template <int reduce_threads, typename Float, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductH##suffix
#define REDUCE_TYPES Float *a, Float2 *b, int stride
#define REDUCE_PARAMS a, b, stride
#define REDUCE_REAL_AUXILIARY(i)					\
  READ_HALF_SPINOR(a, texHalf1, stride);				\
  READ_HALF_SPINOR(b, texHalf2, stride);				\
  REAL_DOT_FLOAT4(rdot0, a0, b0);					\
  REAL_DOT_FLOAT4(rdot1, a1, b1);					\
  REAL_DOT_FLOAT4(rdot2, a2, b2);					\
  REAL_DOT_FLOAT4(rdot3, a3, b3);					\
  REAL_DOT_FLOAT4(rdot4, a4, b4);					\
  REAL_DOT_FLOAT4(rdot5, a5, b5);					\
  rdot0 += rdot1; rdot2 += rdot3; rdot4 += rdot5; rdot0 += rdot2; rdot0 += rdot4;
#define REDUCE_IMAG_AUXILIARY(i)					\
  IMAG_DOT_FLOAT4(idot0, a0, b0);					\
  IMAG_DOT_FLOAT4(idot1, a1, b1);					\
  IMAG_DOT_FLOAT4(idot2, a2, b2);					\
  IMAG_DOT_FLOAT4(idot3, a3, b3);					\
  IMAG_DOT_FLOAT4(idot4, a4, b4);					\
  IMAG_DOT_FLOAT4(idot5, a5, b5);					\
  idot0 += idot1; idot2 += idot3; idot4 += idot5; idot0 += idot2; idot0 += idot4;
#define REDUCE_REAL_OPERATION(i) (ac*bc*rdot0)
#define REDUCE_IMAG_OPERATION(i) (ac*bc*idot0)
#include "reduce_complex_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_REAL_AUXILIARY
#undef REDUCE_IMAG_AUXILIARY
#undef REDUCE_REAL_OPERATION
#undef REDUCE_IMAG_OPERATION

double2 cDotProductCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  blas_quda_flops += 4*x.real_length;
  checkSpinor(x,y);
  int length = x.length/2;
  blas_quda_bytes += 2*x.real_length*x.precision;
  if (x.precision == QUDA_DOUBLE_PRECISION) {
    char c = 0;
    int spinor_bytes = x.length*sizeof(double);
    cudaBindTexture(0, xTexDouble2, x.v, spinor_bytes); 
    cudaBindTexture(0, yTexDouble2, y.v, spinor_bytes); 
    return cDotProductDCuda((double2*)x.v, (double2*)y.v, c, length, 17, x.precision);
  } else if (x.precision == QUDA_SINGLE_PRECISION) {
    char c = 0;
    int spinor_bytes = x.length*sizeof(float);
    cudaBindTexture(0, xTexSingle2, x.v, spinor_bytes); 
    cudaBindTexture(0, yTexSingle2, y.v, spinor_bytes); 
    return cDotProductSCuda((float2*)x.v, (float2*)y.v, c, length, 17, x.precision);
  } else {
    int spinor_bytes = x.length*sizeof(short);
    cudaBindTexture(0, texHalf1, x.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, x.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf2, y.v, spinor_bytes); 
    cudaBindTexture(0, texNorm2, y.norm, spinor_bytes/12);    
    blas_quda_bytes += (2*x.real_length*x.precision) / (x.nColor * x.nSpin);
    return cDotProductHCuda((float*)x.norm, (float*)y.norm, x.stride, x.volume, 17, x.precision);
  }
}

//
// double2 xpaycDotzyCuda(float2 *x, float a, float2 *y, float2 *z, int n) {}
//
// First performs the operation y = x + a*y
// Second returns complex dot product (z,y)
//

template <int reduce_threads, typename Float, typename Float2>
#define REDUCE_FUNC_NAME(suffix) xpaycDotzyD##suffix
#define REDUCE_TYPES Float2 *x, Float a, Float2 *y, Float2 *z
#define REDUCE_PARAMS x, a, y, z
#define REDUCE_REAL_AUXILIARY(i)		\
  Float2 X = READ_DOUBLE2_TEXTURE(x, i);	\
  Float2 Y = READ_DOUBLE2_TEXTURE(y, i);	\
  Float2 Z = READ_DOUBLE2_TEXTURE(z, i);
#define REDUCE_IMAG_AUXILIARY(i) y[i].x = X.x + a*Y.x; y[i].y = X.y + a*Y.y
#define REDUCE_REAL_OPERATION(i) (Z.x*y[i].x + Z.y*y[i].y)
#define REDUCE_IMAG_OPERATION(i) (Z.x*y[i].y - Z.y*y[i].x)
#include "reduce_complex_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_REAL_AUXILIARY
#undef REDUCE_IMAG_AUXILIARY
#undef REDUCE_REAL_OPERATION
#undef REDUCE_IMAG_OPERATION

template <int reduce_threads, typename Float, typename Float2>
#define REDUCE_FUNC_NAME(suffix) xpaycDotzyS##suffix
#define REDUCE_TYPES Float2 *x, Float a, Float2 *y, Float2 *z
#define REDUCE_PARAMS x, a, y, z
#define REDUCE_REAL_AUXILIARY(i) y[i].x = x[i].x + a*y[i].x
#define REDUCE_IMAG_AUXILIARY(i) y[i].y = x[i].y + a*y[i].y
#define REDUCE_REAL_OPERATION(i) (z[i].x*y[i].x + z[i].y*y[i].y)
#define REDUCE_IMAG_OPERATION(i) (z[i].x*y[i].y - z[i].y*y[i].x)
#include "reduce_complex_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_REAL_AUXILIARY
#undef REDUCE_IMAG_AUXILIARY
#undef REDUCE_REAL_OPERATION
#undef REDUCE_IMAG_OPERATION

template <int reduce_threads, typename Float, typename Float2>
#define REDUCE_FUNC_NAME(suffix) xpaycDotzyH##suffix
#define REDUCE_TYPES Float a, short4 *yH, Float2 *yN, int stride
#define REDUCE_PARAMS a, yH, yN, stride
#define REDUCE_REAL_AUXILIARY(i)					\
  RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);		\
  RECONSTRUCT_HALF_SPINOR(z, texHalf3, texNorm3, stride);		\
  XPAY_FLOAT4(x0, a, y0);						\
  XPAY_FLOAT4(x1, a, y1);						\
  XPAY_FLOAT4(x2, a, y2);						\
  XPAY_FLOAT4(x3, a, y3);						\
  XPAY_FLOAT4(x4, a, y4);						\
  XPAY_FLOAT4(x5, a, y5);						\
  REAL_DOT_FLOAT4(rdot0, z0, y0);					\
  REAL_DOT_FLOAT4(rdot1, z1, y1);					\
  REAL_DOT_FLOAT4(rdot2, z2, y2);					\
  REAL_DOT_FLOAT4(rdot3, z3, y3);					\
  REAL_DOT_FLOAT4(rdot4, z4, y4);					\
  REAL_DOT_FLOAT4(rdot5, z5, y5);					\
  rdot0 += rdot1; rdot2 += rdot3; rdot4 += rdot5; rdot0 += rdot2; rdot0 += rdot4;
#define REDUCE_IMAG_AUXILIARY(i)					\
  IMAG_DOT_FLOAT4(idot0, z0, y0);					\
  IMAG_DOT_FLOAT4(idot1, z1, y1);					\
  IMAG_DOT_FLOAT4(idot2, z2, y2);					\
  IMAG_DOT_FLOAT4(idot3, z3, y3);					\
  IMAG_DOT_FLOAT4(idot4, z4, y4);					\
  IMAG_DOT_FLOAT4(idot5, z5, y5);					\
  idot0 += idot1; idot2 += idot3; idot4 += idot5; idot0 += idot2; idot0 += idot4; \
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);
#define REDUCE_REAL_OPERATION(i) (rdot0)
#define REDUCE_IMAG_OPERATION(i) (idot0)
#include "reduce_complex_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_REAL_AUXILIARY
#undef REDUCE_IMAG_AUXILIARY
#undef REDUCE_REAL_OPERATION
#undef REDUCE_IMAG_OPERATION

double2 xpaycDotzyCuda(cudaColorSpinorField &x, const double &a, cudaColorSpinorField &y, cudaColorSpinorField &z) {
  blas_quda_flops += 6*x.real_length;
  checkSpinor(x,y);
  checkSpinor(x,z);
  int length = x.length/2;
  blas_quda_bytes += 4*x.real_length*x.precision;
  if (x.precision == QUDA_DOUBLE_PRECISION) {
    int spinor_bytes = x.length*sizeof(double);
    cudaBindTexture(0, xTexDouble2, x.v, spinor_bytes); 
    cudaBindTexture(0, yTexDouble2, y.v, spinor_bytes); 
    cudaBindTexture(0, zTexDouble2, z.v, spinor_bytes); 
    return xpaycDotzyDCuda((double2*)x.v, a, (double2*)y.v, (double2*)z.v, length, 18, x.precision);
  } else if (x.precision == QUDA_SINGLE_PRECISION) {
    return xpaycDotzySCuda((float2*)x.v, (float)a, (float2*)y.v, (float2*)z.v, length, 18, x.precision);
  } else {
    int spinor_bytes = x.length*sizeof(short);
    cudaBindTexture(0, texHalf1, x.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, x.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf2, y.v, spinor_bytes); 
    cudaBindTexture(0, texNorm2, y.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf3, z.v, spinor_bytes); 
    cudaBindTexture(0, texNorm3, z.norm, spinor_bytes/12);    
    blas_quda_bytes += (4*x.real_length*x.precision) / (x.nColor * x.nSpin);
    return xpaycDotzyHCuda((float)a, (short4*)y.v, (float*)y.norm, x.stride, x.volume, 18, x.precision);
  }
}


//
// double3 cDotProductNormACuda(float2 *a, float2 *b, int n) {}
//
template <int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductNormAD##suffix
#define REDUCE_TYPES Float2 *x, Float2 *y
#define REDUCE_PARAMS x, y
#define REDUCE_X_AUXILIARY(i) Float2 a = READ_DOUBLE2_TEXTURE(x, i);
#define REDUCE_Y_AUXILIARY(i) Float2 b = READ_DOUBLE2_TEXTURE(y, i);
#define REDUCE_Z_AUXILIARY(i)
#define REDUCE_X_OPERATION(i) (a.x*b.x + a.y*b.y)
#define REDUCE_Y_OPERATION(i) (a.x*b.y - a.y*b.x)
#define REDUCE_Z_OPERATION(i) (a.x*a.x + a.y*a.y)
#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

template <int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductNormAS##suffix
#define REDUCE_TYPES Float2 *a, Float2 *b
#define REDUCE_PARAMS a, b
#define REDUCE_X_AUXILIARY(i)
#define REDUCE_Y_AUXILIARY(i)
#define REDUCE_Z_AUXILIARY(i)
#define REDUCE_X_OPERATION(i) (a[i].x*b[i].x + a[i].y*b[i].y)
#define REDUCE_Y_OPERATION(i) (a[i].x*b[i].y - a[i].y*b[i].x)
#define REDUCE_Z_OPERATION(i) (a[i].x*a[i].x + a[i].y*a[i].y)
#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

template <int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductNormAH##suffix
#define REDUCE_TYPES Float2 *x, Float2 *y, int stride
#define REDUCE_PARAMS x, y, stride
#define REDUCE_X_AUXILIARY(i)						\
  READ_HALF_SPINOR(x, texHalf1, stride);				\
  READ_HALF_SPINOR(y, texHalf2, stride);				\
  REAL_DOT_FLOAT4(norm0, x0, x0);					\
  REAL_DOT_FLOAT4(norm1, x1, x1);					\
  REAL_DOT_FLOAT4(norm2, x2, x2);					\
  REAL_DOT_FLOAT4(norm3, x3, x3);					\
  REAL_DOT_FLOAT4(norm4, x4, x4);					\
  REAL_DOT_FLOAT4(norm5, x5, x5);					\
  norm0 += norm1; norm2 += norm3; norm4 += norm5; norm0 += norm2, norm0 += norm4;
#define REDUCE_Y_AUXILIARY(i)						\
  REAL_DOT_FLOAT4(rdot0, x0, y0);					\
  REAL_DOT_FLOAT4(rdot1, x1, y1);					\
  REAL_DOT_FLOAT4(rdot2, x2, y2);					\
  REAL_DOT_FLOAT4(rdot3, x3, y3);					\
  REAL_DOT_FLOAT4(rdot4, x4, y4);					\
  REAL_DOT_FLOAT4(rdot5, x5, y5);					\
  rdot0 += rdot1; rdot2 += rdot3; rdot4 += rdot5; rdot0 += rdot2; rdot0 += rdot4;
#define REDUCE_Z_AUXILIARY(i)						\
  IMAG_DOT_FLOAT4(idot0, x0, y0);					\
  IMAG_DOT_FLOAT4(idot1, x1, y1);					\
  IMAG_DOT_FLOAT4(idot2, x2, y2);					\
  IMAG_DOT_FLOAT4(idot3, x3, y3);					\
  IMAG_DOT_FLOAT4(idot4, x4, y4);					\
  IMAG_DOT_FLOAT4(idot5, x5, y5);					\
  idot0 += idot1; idot2 += idot3; idot4 += idot5; idot0 += idot2; idot0 += idot4;  
#define REDUCE_X_OPERATION(i) (xc*yc*rdot0)
#define REDUCE_Y_OPERATION(i) (xc*yc*idot0)
#define REDUCE_Z_OPERATION(i) (xc*xc*norm0)
#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

double3 cDotProductNormACuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  blas_quda_flops += 6*x.real_length;
  checkSpinor(x,y);
  int length = x.length/2;
  blas_quda_bytes += 2*x.real_length*x.precision;
  if (x.precision == QUDA_DOUBLE_PRECISION) {
    int spinor_bytes = x.length*sizeof(double);
    cudaBindTexture(0, xTexDouble2, x.v, spinor_bytes); 
    cudaBindTexture(0, yTexDouble2, y.v, spinor_bytes); 
    return cDotProductNormADCuda((double2*)x.v, (double2*)y.v, length, 19, x.precision);
  } else if (x.precision == QUDA_SINGLE_PRECISION) {
    return cDotProductNormASCuda((float2*)x.v, (float2*)y.v, length, 19, x.precision);
  } else {
    int spinor_bytes = x.length*sizeof(short);
    cudaBindTexture(0, texHalf1, x.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, x.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf2, y.v, spinor_bytes); 
    cudaBindTexture(0, texNorm2, y.norm, spinor_bytes/12);    
    blas_quda_bytes += (2*x.real_length*x.precision) / (x.nColor * x.nSpin);
    return cDotProductNormAHCuda((float*)x.norm, (float*)y.norm, x.stride, x.volume, 19, x.precision);
  }
}

//
// double3 cDotProductNormBCuda(float2 *a, float2 *b, int n) {}
//
template <int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductNormBD##suffix
#define REDUCE_TYPES Float2 *x, Float2 *y
#define REDUCE_PARAMS x, y
#define REDUCE_X_AUXILIARY(i) Float2 a = READ_DOUBLE2_TEXTURE(x, i);
#define REDUCE_Y_AUXILIARY(i) Float2 b = READ_DOUBLE2_TEXTURE(y, i);
#define REDUCE_Z_AUXILIARY(i)
#define REDUCE_X_OPERATION(i) (a.x*b.x + a.y*b.y)
#define REDUCE_Y_OPERATION(i) (a.x*b.y - a.y*b.x)
#define REDUCE_Z_OPERATION(i) (b.x*b.x + b.y*b.y)
#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

template <int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductNormBS##suffix
#define REDUCE_TYPES Float2 *a, Float2 *b
#define REDUCE_PARAMS a, b
#define REDUCE_X_AUXILIARY(i)
#define REDUCE_Y_AUXILIARY(i)
#define REDUCE_Z_AUXILIARY(i)
#define REDUCE_X_OPERATION(i) (a[i].x*b[i].x + a[i].y*b[i].y)
#define REDUCE_Y_OPERATION(i) (a[i].x*b[i].y - a[i].y*b[i].x)
#define REDUCE_Z_OPERATION(i) (b[i].x*b[i].x + b[i].y*b[i].y)
#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

template <int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductNormBH##suffix
#define REDUCE_TYPES Float2 *x, Float2 *y, int stride
#define REDUCE_PARAMS x, y, stride
#define REDUCE_X_AUXILIARY(i)						\
  READ_HALF_SPINOR(x, texHalf1, stride);				\
  READ_HALF_SPINOR(y, texHalf2, stride);				\
  REAL_DOT_FLOAT4(norm0, y0, y0);					\
  REAL_DOT_FLOAT4(norm1, y1, y1);					\
  REAL_DOT_FLOAT4(norm2, y2, y2);					\
  REAL_DOT_FLOAT4(norm3, y3, y3);					\
  REAL_DOT_FLOAT4(norm4, y4, y4);					\
  REAL_DOT_FLOAT4(norm5, y5, y5);					\
  norm0 += norm1; norm2 += norm3; norm4 += norm5; norm0 += norm2, norm0 += norm4;
#define REDUCE_Y_AUXILIARY(i)						\
  REAL_DOT_FLOAT4(rdot0, x0, y0);					\
  REAL_DOT_FLOAT4(rdot1, x1, y1);					\
  REAL_DOT_FLOAT4(rdot2, x2, y2);					\
  REAL_DOT_FLOAT4(rdot3, x3, y3);					\
  REAL_DOT_FLOAT4(rdot4, x4, y4);					\
  REAL_DOT_FLOAT4(rdot5, x5, y5);					\
  rdot0 += rdot1; rdot2 += rdot3; rdot4 += rdot5; rdot0 += rdot2; rdot0 += rdot4;
#define REDUCE_Z_AUXILIARY(i)						\
  IMAG_DOT_FLOAT4(idot0, x0, y0);					\
  IMAG_DOT_FLOAT4(idot1, x1, y1);					\
  IMAG_DOT_FLOAT4(idot2, x2, y2);					\
  IMAG_DOT_FLOAT4(idot3, x3, y3);					\
  IMAG_DOT_FLOAT4(idot4, x4, y4);					\
  IMAG_DOT_FLOAT4(idot5, x5, y5);					\
  idot0 += idot1; idot2 += idot3; idot4 += idot5; idot0 += idot2; idot0 += idot4;  
#define REDUCE_X_OPERATION(i) (xc*yc*rdot0)
#define REDUCE_Y_OPERATION(i) (xc*yc*idot0)
#define REDUCE_Z_OPERATION(i) (yc*yc*norm0)
#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

double3 cDotProductNormBCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  blas_quda_flops += 6*x.real_length;
  checkSpinor(x,y);
  int length = x.length/2;
  blas_quda_bytes += 2*x.real_length*x.precision;
  if (x.precision == QUDA_DOUBLE_PRECISION) {
    int spinor_bytes = x.length*sizeof(double);
    cudaBindTexture(0, xTexDouble2, x.v, spinor_bytes); 
    cudaBindTexture(0, yTexDouble2, y.v, spinor_bytes); 
    return cDotProductNormBDCuda((double2*)x.v, (double2*)y.v, length, 20, x.precision);
  } else if (x.precision == QUDA_SINGLE_PRECISION) {
    return cDotProductNormBSCuda((float2*)x.v, (float2*)y.v, length, 20, x.precision);
  } else {
    int spinor_bytes = x.length*sizeof(short);
    cudaBindTexture(0, texHalf1, x.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, x.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf2, y.v, spinor_bytes); 
    cudaBindTexture(0, texNorm2, y.norm, spinor_bytes/12);    
    blas_quda_bytes += (2*x.real_length*x.precision) / (x.nColor * x.nSpin);
    return cDotProductNormBHCuda((float*)x.norm, (float*)y.norm, x.stride, x.volume, 20, x.precision);
  }
}


//
// double3 caxpbypzYmbwcDotProductWYNormYCuda(float2 a, float2 *x, float2 b, float2 *y, 
// float2 *z, float2 *w, float2 *u, int len)
//
template <int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) caxpbypzYmbwcDotProductWYNormYD##suffix
#define REDUCE_TYPES Float2 a, Float2 *x, Float2 b, Float2 *y, Float2 *z, Float2 *w, Float2 *u
#define REDUCE_PARAMS a, x, b, y, z, w, u
#define REDUCE_X_AUXILIARY(i)				\
  Float2 X = READ_DOUBLE2_TEXTURE(x, i);		\
  Float2 Y = READ_DOUBLE2_TEXTURE(y, i);		\
  Float2 W = READ_DOUBLE2_TEXTURE(w, i);				
#define REDUCE_Y_AUXILIARY(i)			\
  Float2 Z = read_Float2(z, i);			\
  Z.x += a.x*X.x - a.y*X.y;			\
  Z.y += a.y*X.x + a.x*X.y;			\
  Z.x += b.x*Y.x - b.y*Y.y;			\
  Z.y += b.y*Y.x + b.x*Y.y;			\
  Y.x -= b.x*W.x - b.y*W.y;			\
  Y.y -= b.y*W.x + b.x*W.y;	
#define REDUCE_Z_AUXILIARY(i)	      \
  z[i] = make_Float2(Z);	      \
  y[i] = make_Float2(Y);	      
#define REDUCE_X_OPERATION(i) (u[i].x*y[i].x + u[i].y*y[i].y)
#define REDUCE_Y_OPERATION(i) (u[i].x*y[i].y - u[i].y*y[i].x)
#define REDUCE_Z_OPERATION(i) (y[i].x*y[i].x + y[i].y*y[i].y)
#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

template <int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) caxpbypzYmbwcDotProductWYNormYS##suffix
#define REDUCE_TYPES Float2 a, Float2 *x, Float2 b, Float2 *y, Float2 *z, Float2 *w, Float2 *u
#define REDUCE_PARAMS a, x, b, y, z, w, u
#define REDUCE_X_AUXILIARY(i)				\
  Float2 X = read_Float2(x, i);		\
  Float2 Y = read_Float2(y, i);		\
  Float2 W = read_Float2(w, i);		
#define REDUCE_Y_AUXILIARY(i)			\
  Float2 Z = read_Float2(z, i);	\
  Z.x += a.x*X.x - a.y*X.y;			\
  Z.y += a.y*X.x + a.x*X.y;			\
  Z.x += b.x*Y.x - b.y*Y.y;			\
  Z.y += b.y*Y.x + b.x*Y.y;			\
  Y.x -= b.x*W.x - b.y*W.y;			\
  Y.y -= b.y*W.x + b.x*W.y;	
#define REDUCE_Z_AUXILIARY(i)	      \
  z[i] = make_Float2(Z);	      \
  y[i] = make_Float2(Y);	      
#define REDUCE_X_OPERATION(i) (u[i].x*y[i].x + u[i].y*y[i].y)
#define REDUCE_Y_OPERATION(i) (u[i].x*y[i].y - u[i].y*y[i].x)
#define REDUCE_Z_OPERATION(i) (y[i].x*y[i].x + y[i].y*y[i].y)
#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

//
// double3 caxpbypzYmbwcDotProductWYNormYCuda(float2 a, float2 *x, float2 b, float2 *y, 
// float2 *z, float2 *w, float2 *u, int len)
//
template <int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) caxpbypzYmbwcDotProductWYNormYH##suffix
#define REDUCE_TYPES Float2 a, Float2 b, short4 *yH, float *yN, short4 *zH, float *zN, float *u, int stride
#define REDUCE_PARAMS a, b, yH, yN, zH, zN, u, stride
#define REDUCE_X_AUXILIARY(i)						\
  RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);		\
  RECONSTRUCT_HALF_SPINOR(z, texHalf3, texNorm3, stride);		\
  CAXPBYPZ_FLOAT4(a, x0, b, y0, z0);					\
  CAXPBYPZ_FLOAT4(a, x1, b, y1, z1);					\
  CAXPBYPZ_FLOAT4(a, x2, b, y2, z2);					\
  CAXPBYPZ_FLOAT4(a, x3, b, y3, z3);					\
  CAXPBYPZ_FLOAT4(a, x4, b, y4, z4);					\
  CAXPBYPZ_FLOAT4(a, x5, b, y5, z5);					\
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE(zH, zN, z, stride);			\
  RECONSTRUCT_HALF_SPINOR(w, texHalf4, texNorm4, stride);		\
  CMAXPY_FLOAT4(b, w0, y0);						\
  CMAXPY_FLOAT4(b, w1, y1);						\
  CMAXPY_FLOAT4(b, w2, y2);						\
  CMAXPY_FLOAT4(b, w3, y3);						\
  CMAXPY_FLOAT4(b, w4, y4);						\
  CMAXPY_FLOAT4(b, w5, y5);						\
  REAL_DOT_FLOAT4(norm0, y0, y0);					\
  REAL_DOT_FLOAT4(norm1, y1, y1);					\
  REAL_DOT_FLOAT4(norm2, y2, y2);					\
  REAL_DOT_FLOAT4(norm3, y3, y3);					\
  REAL_DOT_FLOAT4(norm4, y4, y4);					\
  REAL_DOT_FLOAT4(norm5, y5, y5);					\
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);			
#define REDUCE_Y_AUXILIARY(i)						\
  READ_HALF_SPINOR(u, texHalf5, stride);				\
  REAL_DOT_FLOAT4(rdot0, u0, y0);					\
  REAL_DOT_FLOAT4(rdot1, u1, y1);					\
  REAL_DOT_FLOAT4(rdot2, u2, y2);					\
  REAL_DOT_FLOAT4(rdot3, u3, y3);					\
  REAL_DOT_FLOAT4(rdot4, u4, y4);					\
  REAL_DOT_FLOAT4(rdot5, u5, y5);					\
  IMAG_DOT_FLOAT4(idot0, u0, y0);					\
  IMAG_DOT_FLOAT4(idot1, u1, y1);					\
  IMAG_DOT_FLOAT4(idot2, u2, y2);					\
  IMAG_DOT_FLOAT4(idot3, u3, y3);					\
  IMAG_DOT_FLOAT4(idot4, u4, y4);					\
  IMAG_DOT_FLOAT4(idot5, u5, y5);					
#define REDUCE_Z_AUXILIARY(i)						\
  norm0 += norm1; norm2 += norm3; norm4 += norm5; norm0 += norm2, norm0 += norm4; \
  rdot0 += rdot1; rdot2 += rdot3; rdot4 += rdot5; rdot0 += rdot2; rdot0 += rdot4; \
  idot0 += idot1; idot2 += idot3; idot4 += idot5; idot0 += idot2; idot0 += idot4; 
#define REDUCE_X_OPERATION(i) (uc*rdot0)
#define REDUCE_Y_OPERATION(i) (uc*idot0)
#define REDUCE_Z_OPERATION(i) (norm0)
#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

// This convoluted kernel does the following: z += a*x + b*y, y -= b*w, norm = (y,y), dot = (u, y)
double3 caxpbypzYmbwcDotProductWYNormYCuda(const double2 &a, cudaColorSpinorField &x, const double2 &b, cudaColorSpinorField &y,
					   cudaColorSpinorField &z, cudaColorSpinorField &w, cudaColorSpinorField &u) {
  blas_quda_flops += 18*x.real_length;
  checkSpinor(x,y);
  checkSpinor(x,z);
  checkSpinor(x,w);
  checkSpinor(x,u);
  int length = x.length/2;
  blas_quda_bytes += 7*x.real_length*x.precision;
  if (x.precision == QUDA_DOUBLE_PRECISION) {
    int spinor_bytes = x.length*sizeof(double);
    cudaBindTexture(0, xTexDouble2, x.v, spinor_bytes); 
    cudaBindTexture(0, yTexDouble2, y.v, spinor_bytes); 
    cudaBindTexture(0, zTexDouble2, z.v, spinor_bytes); 
    cudaBindTexture(0, wTexDouble2, w.v, spinor_bytes); 
    cudaBindTexture(0, uTexDouble2, u.v, spinor_bytes); 
    return caxpbypzYmbwcDotProductWYNormYDCuda(a, (double2*)x.v, b, (double2*)y.v, (double2*)z.v, 
					       (double2*)w.v, (double2*)u.v, length, 21, x.precision);
  } else if (x.precision == QUDA_SINGLE_PRECISION) {
    float2 af2 = make_float2((float)a.x, (float)a.y);
    float2 bf2 = make_float2((float)b.x, (float)b.y);
    return caxpbypzYmbwcDotProductWYNormYSCuda(af2, (float2*)x.v, bf2, (float2*)y.v, (float2*)z.v,
					       (float2*)w.v, (float2*)u.v, length, 21, x.precision);
  } else {
    int spinor_bytes = x.length*sizeof(short);
    cudaBindTexture(0, texHalf1, x.v, spinor_bytes); 
    cudaBindTexture(0, texNorm1, x.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf2, y.v, spinor_bytes); 
    cudaBindTexture(0, texNorm2, y.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf3, z.v, spinor_bytes); 
    cudaBindTexture(0, texNorm3, z.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf4, w.v, spinor_bytes); 
    cudaBindTexture(0, texNorm4, w.norm, spinor_bytes/12);    
    cudaBindTexture(0, texHalf5, u.v, spinor_bytes); 
    cudaBindTexture(0, texNorm5, u.norm, spinor_bytes/12);    
    float2 af2 = make_float2((float)a.x, (float)a.y);
    float2 bf2 = make_float2((float)b.x, (float)b.y);
    blas_quda_bytes += (7*x.real_length*x.precision) / (x.nColor * x.nSpin);
    return caxpbypzYmbwcDotProductWYNormYHCuda(af2, bf2, (short4*)y.v, (float*)y.norm, 
					       (short4*)z.v, (float*)z.norm, (float*)u.norm, 
					       y.stride, y.volume, 21, x.precision);
  }
}

